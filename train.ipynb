{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vt5hanGL4WpD"
   },
   "source": [
    "# Train a deep CNN on XPS data on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTjEnuR-LwEo"
   },
   "source": [
    "In this notebook, we will train a deep convolutional network on iron XPS spectra made up of linear combinations of single iron reference spectra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6EVGrGFLwEr"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU2aEkqdLwE_"
   },
   "source": [
    "### Mount google drive, change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16129,
     "status": "ok",
     "timestamp": 1613636373618,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "5iL11_yXLwFB",
    "outputId": "5a0ff398-66a3-46da-b483-29e90da595b6"
   },
   "outputs": [],
   "source": [
    "# Mount drive\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Change working path\n",
    "os.chdir('/content/drive/My Drive/deepxps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC3yXYXaLwEt"
   },
   "source": [
    "### Install packages and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 87692,
     "status": "ok",
     "timestamp": 1613636458384,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "s4MxYB_b33V9"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install packages\n",
    "!pip install python-docx\n",
    "!pip install tensorflow==2.3.0 as tf\n",
    "\n",
    "# Import standard modules and magic commands\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pytz\n",
    "import importlib\n",
    "\n",
    "# Magic commands\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Disable tf warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad6lrbtPUwVk"
   },
   "source": [
    "### Check TensorFlow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 439,
     "status": "ok",
     "timestamp": 1613636458840,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "1wemeM47_Dsy",
    "outputId": "1911d009-df58-459b-af71-d56f98724626"
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMuIKSLjUeBO"
   },
   "source": [
    "### Check TPU connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1613638036420,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "2wcWtXa1UcSP",
    "outputId": "864fe175-a218-4129-912d-e0fb089d3404"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.profiler import profiler_client\n",
    "\n",
    "tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n",
    "print(profiler_client.monitor(tpu_profile_service_address, 100, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R1BPxWcLLwFp"
   },
   "source": [
    "## Initial training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qQx0QfuLwFQ"
   },
   "source": [
    "### Load custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2799,
     "status": "ok",
     "timestamp": 1613636461213,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "SfLJzbL04VZ8",
    "outputId": "cfad7ba3-edca-4ca7-919e-d101b17a6c9b"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    importlib.reload(classifier)\n",
    "    importlib.reload(clfutils)\n",
    "    print('Modules were reloaded.')\n",
    "except:\n",
    "    import xpsdeeplearning.network.classifier as classifier\n",
    "    import xpsdeeplearning.network.utils as clfutils\n",
    "    print('Modules were loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7M70DZczykg"
   },
   "source": [
    "### Set up the parameters & folder structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3077,
     "status": "ok",
     "timestamp": 1613636461502,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "GXy2WdXYXcXc",
    "outputId": "a6950810-935f-498b-829a-3cc74d16a1d7"
   },
   "outputs": [],
   "source": [
    "np.random.seed(502)\n",
    "time = datetime.datetime.now().astimezone(pytz.timezone('Europe/Berlin')).strftime(\"%Y%m%d_%Hh%Mm\")\n",
    "exp_name = 'Fe_4_classes_linear_comb_new_noise_small_resnet'\n",
    "\n",
    "clf = classifier.Classifier(time = time,\n",
    "                            exp_name = exp_name,\n",
    "                            task = 'regression',\n",
    "                            intensity_only = True)\n",
    "\n",
    "### If labels not saved with data ###\n",
    "labels = []\n",
    "clf = classifier.Classifier(time = time,\n",
    "                            exp_name = exp_name,\n",
    "                            task = 'regression',\n",
    "                            intensity_only = True,\n",
    "                            labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0wkRVOuLwFy"
   },
   "source": [
    "### Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "executionInfo": {
     "elapsed": 26773,
     "status": "ok",
     "timestamp": 1613636485211,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "KZIiyOyKtxmf",
    "outputId": "c3e3f059-ea40-4ff7-9f89-a955b4fe4a24"
   },
   "outputs": [],
   "source": [
    "input_filepath = r'/content/drive/My Drive/deepxps/datasets/20210222_Fe_linear_combination_small_gas_phase.h5'\n",
    "\n",
    "train_test_split = 0.2\n",
    "train_val_split = 0.2\n",
    "no_of_examples = 100000\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test,\\\n",
    "    aug_values_train, aug_values_val, aug_values_test =\\\n",
    "        clf.load_data_preprocess(input_filepath = input_filepath,\n",
    "                                 no_of_examples = no_of_examples,\n",
    "                                 train_test_split = train_test_split,\n",
    "                                 train_val_split = train_val_split)\n",
    "               \n",
    "# Check how the examples are distributed across the classes.\n",
    "class_distribution = clf.datahandler.check_class_distribution(clf.task)\n",
    "clf.plot_class_distribution()\n",
    "clf.plot_random(no_of_spectra = 10, dataset = 'train')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKXmi1LMLwF6"
   },
   "source": [
    "### Design the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 444,
     "status": "ok",
     "timestamp": 1613636511676,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "pyxJ9_qh5awz",
    "outputId": "4fc8ebe5-cef1-4973-85e7-a366cfc9e527"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    importlib.reload(models)\n",
    "    print('Models module was reloaded.')\n",
    "except:\n",
    "    import xpsdeeplearning.network.models as models\n",
    "    print('Models module was loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 520,
     "status": "ok",
     "timestamp": 1613637509928,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "G9_iGDpjOWhK"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "\n",
    "class ResNet1D(models.EmptyModel):\n",
    "    \"\"\"\n",
    "    Instantiates the ResNet50 architecture in 1D similar to the original \n",
    "    ResNet paper. Using the functional API in Keras.\n",
    "    \n",
    "    Implementation of the popular ResNet50 the following architecture:\n",
    "    CONV1D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 ->\n",
    "    CONVBLOCK -> IDBLOCK*3 -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> \n",
    "    IDBLOCK*2 -> AVGPOOL (optional) -> OUTPUTLAYER\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 inputshape,\n",
    "                 num_classes,\n",
    "                 ap=False,\n",
    "                 no_of_inputs=1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_classes : int\n",
    "            Number of output classes.\n",
    "        ap : bool, optional\n",
    "            If ap, then an AveragePooling1D layer is added after the\n",
    "            residual blocks. The default is False.\n",
    "        no_of_inputs : int, optional\n",
    "            Number of input layers. The default is 1.\n",
    "            (not working here)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None.\n",
    "\n",
    "        \"\"\"\n",
    "        self.ap = ap\n",
    "        \n",
    "        self.input_1 = layers.Input(shape=inputshape,\n",
    "                                    name='input_1')\n",
    "    \n",
    "        # Zero-Padding\n",
    "        self.zero_pad_1 = layers.ZeroPadding1D(padding=3)(self.input_1)\n",
    "        \n",
    "        # Stage 1\n",
    "        self.conv_1 = layers.Conv1D(filters=64,\n",
    "                                    kernel_size=2,\n",
    "                                    padding='valid',\n",
    "                                    kernel_initializer=glorot_uniform(seed=0),\n",
    "                                    name='stage1_conv')(self.zero_pad_1)\n",
    "        self.batch_1 = layers.BatchNormalization(\n",
    "            axis=1,\n",
    "            name='stage1_bn')(self.conv_1)\n",
    "        self.act_1 = layers.Activation(\n",
    "            activation='relu',\n",
    "            name='stage1_act')(self.batch_1)\n",
    "        self.max_pool_1 = layers.MaxPooling1D(\n",
    "            pool_size=1,\n",
    "            strides=1,\n",
    "            name='stage1_max_pool')(self.act_1)\n",
    "        \n",
    "        # Stage 2\n",
    "        self.conv_block_2a = models.ConvBlock(filters=[32,32,128],\n",
    "                                       kernel_size_2=4,\n",
    "                                       stage=2,\n",
    "                                       block='a')(self.max_pool_1)\n",
    "        self.id_block_2b = models.IdentityBlock(filters=[32,32,128],\n",
    "                                         kernel_size_2=1,\n",
    "                                         stage=2,\n",
    "                                         block='b')(self.conv_block_2a)\n",
    "        self.id_block_2c = models.IdentityBlock(filters=[32,32,128],\n",
    "                                         kernel_size_2=1,\n",
    "                                         stage=2,\n",
    "                                         block='c')(self.id_block_2b)\n",
    "\n",
    "        # Stage 3\n",
    "        self.conv_block_3a = models.ConvBlock(filters=[64,64,256],\n",
    "                                       kernel_size_2=3,\n",
    "                                       stage=3,\n",
    "                                       block='a')(self.id_block_2c)\n",
    "        self.id_block_3b = models.IdentityBlock(filters=[64,64,256],\n",
    "                                         kernel_size_2=3,\n",
    "                                         stage=3,\n",
    "                                         block='b')(self.conv_block_3a)\n",
    "        self.id_block_3c = models.IdentityBlock(filters=[64,64,256],\n",
    "                                         kernel_size_2=3,\n",
    "                                         stage=3,\n",
    "                                         block='c')(self.id_block_3b)\n",
    "        self.id_block_3d = models.IdentityBlock(filters=[64,64,256],\n",
    "                                         kernel_size_2=3,\n",
    "                                         stage=3,\n",
    "                                         block='d')(self.id_block_3c)\n",
    "        \n",
    "# =============================================================================\n",
    "#         # Stage 4\n",
    "#         self.conv_block_4a = models.ConvBlock(filters=[128,128,1024],\n",
    "#                                        kernel_size_2=3,\n",
    "#                                        stage=4,\n",
    "#                                        block='a')(self.id_block_3d)\n",
    "#         self.id_block_4b = models.IdentityBlock(filters=[128,128,1024],\n",
    "#                                          kernel_size_2=3,\n",
    "#                                          stage=4,\n",
    "#                                          block='b')(self.conv_block_4a)\n",
    "#         self.id_block_4c = models.IdentityBlock(filters=[128,128,1024],\n",
    "#                                          kernel_size_2=3,\n",
    "#                                          stage=4,\n",
    "#                                           block='c')(self.id_block_4b)\n",
    "#         self.id_block_4d = models.IdentityBlock(filters=[128,128,1024],\n",
    "#                                          kernel_size_2=3,\n",
    "#                                          stage=4,\n",
    "#                                          block='d')(self.id_block_4c)\n",
    "#         self.id_block_4e = models.IdentityBlock(filters=[128,128,1024],\n",
    "#                                          kernel_size_2=3,\n",
    "#                                          stage=4,\n",
    "#                                          block='e')(self.id_block_4d)\n",
    "#         self.id_block_4f = models.IdentityBlock(filters=[128,128,1024],\n",
    "#                                          kernel_size_2=3,\n",
    "#                                          stage=4,\n",
    "#                                          block='f')(self.id_block_4e)\n",
    "# \n",
    "#         # Stage 5\n",
    "#         self.conv_block_5a = models.ConvBlock(filters=[256,56,1024],\n",
    "#                                        kernel_size_2=1,\n",
    "#                                        stage=5,\n",
    "#                                        block='a')(self.id_block_4f)\n",
    "#         self.id_block_5b = models.IdentityBlock(filters=[256,56,1024],\n",
    "#                                          kernel_size_2=1,\n",
    "#                                          stage=5,\n",
    "#                                          block='b')(self.conv_block_5a)\n",
    "#         self.id_block_5c = models.IdentityBlock(filters=[256,56,1024],\n",
    "#                                          kernel_size_2=1,\n",
    "#                                          stage=5,\n",
    "#                                          block='c')(self.id_block_5b)\n",
    "# =============================================================================\n",
    "\n",
    "        # Average pooling\n",
    "        if self.ap:\n",
    "            self.avg_pool = layers.AveragePooling1D(\n",
    "                pool_size=3,\n",
    "                name='avg_pool')(self.id_block_3d)\n",
    "            self.flatten = layers.Flatten(name='flatten')(self.avg_pool)\n",
    "        \n",
    "        else:\n",
    "            self.flatten = layers.Flatten(name='flatten')(self.id_block_3d)\n",
    "\n",
    "        # output layer\n",
    "        self.dense = layers.Dense(units=num_classes,\n",
    "                                  activation='sigmoid',\n",
    "                                  kernel_initializer = glorot_uniform(seed=0),\n",
    "                                  name='dense')(self.flatten)\n",
    "        \n",
    "        # output norm\n",
    "        self.output_norm = layers.Lambda(\n",
    "            lambda x: x/tf.reshape(K.sum(x, axis=-1),(-1,1)),\n",
    "            name = 'output_norm')(self.dense)\n",
    "        \n",
    "        super(ResNet1D, self).__init__(inputs=self.input_1,\n",
    "                                        outputs=self.output_norm,\n",
    "                                        inputshape=inputshape,\n",
    "                                        num_classes=num_classes,\n",
    "                                        no_of_inputs=no_of_inputs, \n",
    "                                        name ='ResNet1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zlav04xq9Xr6"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "class RegressionCNN(models.EmptyModel):\n",
    "    \"\"\"\n",
    "    A CNN with three convolutional layers of different kernel size at \n",
    "    the beginning. Works well for learning across scales.\n",
    "    \n",
    "    This is to be used for regression on all labels. -> sigmoid \n",
    "    activation in the last layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputshape, num_classes):      \n",
    "        self.input_1 = layers.Input(shape = inputshape)\n",
    "        \n",
    "        self.conv_1_short = layers.Conv1D(filters=12,\n",
    "                                          kernel_size=5,\n",
    "                                          strides=1,\n",
    "                                          padding='same',\n",
    "                                          activation='relu',\n",
    "                                          name='conv_1_short')(self.input_1)\n",
    "        self.conv_1_medium = layers.Conv1D(filters=12,\n",
    "                                           kernel_size=10,\n",
    "                                           strides=1,\n",
    "                                           padding='same',\n",
    "                                           activation='relu',\n",
    "                                           name='conv_1_medium')(self.input_1)\n",
    "        self.conv_1_long = layers.Conv1D(filters=12,\n",
    "                                         kernel_size=15,\n",
    "                                         strides=1,\n",
    "                                         padding='same',\n",
    "                                         activation='relu',\n",
    "                                         name='conv_1_long')(self.input_1)\n",
    "        \n",
    "        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n",
    "        merged_sublayers = layers.concatenate(sublayers)\n",
    "\n",
    "        self.conv_2 = layers.Conv1D(filters=10,\n",
    "                                    kernel_size=5,\n",
    "                                    strides=1,\n",
    "                                    padding='same',\n",
    "                                    activation='relu',\n",
    "                                    name='conv_2')(merged_sublayers)\n",
    "        self.conv_3 = layers.Conv1D(filters=10,\n",
    "                                    kernel_size=5,\n",
    "                                    strides=1,\n",
    "                                    padding='same',\n",
    "                                    activation='relu',\n",
    "                                    name=\"conv_3\")(self.conv_2)\n",
    "        self.average_pool_1 = layers.AveragePooling1D(\n",
    "            name='average_pool_1')(self.conv_3)\n",
    "        \n",
    "        self.flatten_1 = layers.Flatten(name='flatten1')(self.average_pool_1)\n",
    "        self.drop_1 = layers.Dropout(rate=0.2,\n",
    "                                     name='drop_1')(self.flatten_1)\n",
    "        self.dense_1 = layers.Dense(units=4000,\n",
    "                                    activation='relu',\n",
    "                                    name='dense_1')(self.drop_1)\n",
    "        self.dense_2 = layers.Dense(units=num_classes,\n",
    "                                    activation='sigmoid',\n",
    "                                    name='dense_2')(self.dense_1)\n",
    "        \n",
    "        self.output_norm = layers.Lambda(\n",
    "            lambda x: x/tf.reshape(K.sum(x, axis=-1),(-1,1)),\n",
    "            name = 'output_normalization')(self.dense_2)\n",
    "\n",
    "        no_of_inputs = len(sublayers)\n",
    "\n",
    "        super(RegressionCNN, self).__init__(inputs=self.input_1,\n",
    "                                            outputs=self.output_norm,\n",
    "                                            inputshape=inputshape,\n",
    "                                            num_classes=num_classes,\n",
    "                                            no_of_inputs=no_of_inputs,\n",
    "                                            name='RegressionCNN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76OKygMa9Xr9"
   },
   "source": [
    "#### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1148,
     "status": "ok",
     "timestamp": 1613637514267,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "Af3wbjEW9Xr-"
   },
   "outputs": [],
   "source": [
    "clf.model = RegressionCNN(clf.datahandler.input_shape, clf.datahandler.num_classes)\n",
    "# =============================================================================\n",
    "# clf.model = ResNet1D(clf.datahandler.input_shape,\n",
    "#                      clf.datahandler.num_classess,\n",
    "#                      ap=True)\n",
    "# =============================================================================\n",
    "\n",
    "# Alternative: Build model from available models in models.py\n",
    "# =============================================================================\n",
    "# clf.model = models.RegressionCNN(clf.datahandler.input_shape, \n",
    "#                                  clf.datahandler.num_classes)\n",
    "# =============================================================================\n",
    "# =============================================================================\n",
    "# clf.model = models.ResNet1D(clf.datahandler.input_shape,\n",
    "#                             clf.datahandler.num_classess,\n",
    "#                             ap=True)\n",
    "# ============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2SpBC-R9XsA"
   },
   "source": [
    "### Compile and summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 809
    },
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1613637516834,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "vKQZDayn9XsB",
    "outputId": "9d76c3c2-a9af-464d-98c4-1da6c5467536"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "learning_rate = 1e-05\n",
    "optimizer = Adam(learning_rate = learning_rate) \n",
    "\n",
    "if clf.task == 'regression':\n",
    "    mae = MeanAbsoluteError()\n",
    "    clf.model.compile(loss = mae, optimizer = optimizer)\n",
    "    # =============================================================================\n",
    "    # mse = MeanSquaredError()\n",
    "    # clf.model.compile(loss = mse, optimizer = optimizer)\n",
    "    # =============================================================================\n",
    "    \n",
    "if clf.task == 'classification':\n",
    "    categorical_crossentropy = CategoricalCrossentropy()\n",
    "    clf.model.compile(loss = categorical_crossentropy,\n",
    "                      optimizer = optimizer,\n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "# Plot summary and save model plot.\n",
    "clf.summary()\n",
    "clf.save_and_print_model_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oeboNYkNsf7Z"
   },
   "source": [
    "### Show initial predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 511716,
     "status": "ok",
     "timestamp": 1613638034518,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "Tpj40ttsCpEj",
    "outputId": "ced16881-cffe-4897-a202-5c3ffb60332c"
   },
   "outputs": [],
   "source": [
    "pred_train_initial, pred_test_initial = clf.predict()\n",
    "\n",
    "print(y_train[:5])\n",
    "print(pred_train_initial[:5])\n",
    "\n",
    "print(y_test[:5])\n",
    "print(pred_test_initial[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDOgVS_bLwGC"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abV_fFCb5fiZ",
    "outputId": "d81a86c0-9252-4d74-df6f-2e8962763943"
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "batch_size = 32\n",
    "\n",
    "hist = clf.train(checkpoint = True,\n",
    "                 early_stopping = False,\n",
    "                 tb_log = True, \n",
    "                 csv_log = True,\n",
    "                 hyperparam_log = True,\n",
    "                 epochs = epochs, \n",
    "                 batch_size = batch_size,\n",
    "                 verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9uYqsMlLwGJ"
   },
   "source": [
    "### Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ffr2nfZy5nYi"
   },
   "outputs": [],
   "source": [
    "graph = clfutils.TrainingGraphs(clf.history, clf.logging.fig_dir)\n",
    "graph.plot_loss()\n",
    "if clf.task == 'classification':\n",
    "    graph.plot_accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnAnvSXLLwGQ"
   },
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4g_QmUYG5fuT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if clf.task == 'classification':\n",
    "    score = clf.evaluate()\n",
    "    test_loss, test_accuracy = score[0], score[1]\n",
    "    print('Test loss: ' + str(np.round(test_loss, decimals=8)))\n",
    "    print('Test accuracy: ' + str(np.round(test_accuracy, decimals=3)))\n",
    "elif clf.task == 'regression':\n",
    "    test_loss = clf.evaluate()\n",
    "    print('Test loss: ' + str(np.round(test_loss, decimals=8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uCoVCI-LwGY"
   },
   "source": [
    "###  Predict on train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezklX5YY5fyr"
   },
   "outputs": [],
   "source": [
    "pred_train, pred_test = clf.predict()\n",
    "if clf.task == 'classification':\n",
    "    pred_train_classes, pred_test_classes = clf.predict_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWbogghiLwGl"
   },
   "source": [
    "### Show some predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5DGB_OGLwGm"
   },
   "source": [
    "#### 10 random training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keDKIJriLwGn"
   },
   "outputs": [],
   "source": [
    "clf.plot_random(no_of_spectra = 10, dataset = 'train', with_prediction = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PHAS9XILwGr"
   },
   "source": [
    "#### 10 random test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM3ZZf-qLwGs"
   },
   "outputs": [],
   "source": [
    "clf.plot_random(no_of_spectra = 10, dataset = 'test', with_prediction = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROK1zo8xzBZF"
   },
   "source": [
    "### Show wrong/worst predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "id": "uHFC5PWQzE19"
   },
   "outputs": [],
   "source": [
    "if clf.task == 'classification':\n",
    "    clf.show_wrong_classification()\n",
    "elif clf.task == 'regression':\n",
    "    clf.show_worst_predictions(no_of_spectra = 20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwrgvaLpLwG3"
   },
   "source": [
    "### Save model and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2HBdnZSq5f2D"
   },
   "outputs": [],
   "source": [
    "#clf.save_model()\n",
    "clf.shelve_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6B2wITlLwG_"
   },
   "source": [
    "### Generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rPXD0ki5pOp"
   },
   "outputs": [],
   "source": [
    "dir_name = clf.time + '_' + clf.exp_name\n",
    "rep = clfutils.Report(dir_name)  \n",
    "rep.write()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K4mJKWJxWllD"
   },
   "source": [
    "## Continue training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VH95P7yXcCTq"
   },
   "source": [
    "### Load custom modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90830,
     "status": "ok",
     "timestamp": 1613580000599,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "ILECvnh6cCTr",
    "outputId": "c5853fda-0ca6-4316-fa36-682d54aa7bea"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import importlib\n",
    "    importlib.reload(classifier)\n",
    "    importlib.reload(clfutils)\n",
    "    print('\\n Modules were reloaded.')\n",
    "except:\n",
    "    import xpsdeeplearning.network.classifier as classifier\n",
    "    import xpsdeeplearning.network.utils as clfutils\n",
    "    print('\\n Modules were loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qu-oYRDYv93B"
   },
   "source": [
    "### Reload classifier from logpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath = r'/content/drive/My Drive/deepxps/logs/20210224_14h05m_Fe_4_classes_linear_comb_new_noise_small_resnet'\n",
    "clf = classifier.restore_clf_from_logs(logpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hO_5SSXX0O7I"
   },
   "source": [
    "### Load and inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "executionInfo": {
     "elapsed": 116378,
     "status": "ok",
     "timestamp": 1613580027965,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "bXXFcFUCWxFI",
    "outputId": "4dd29fe1-314b-4540-a16a-2e38184c4cac"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test,\\\n",
    "    aug_values_train, aug_values_val, aug_values_test =\\\n",
    "        clf.load_data_preprocess(input_filepath = clf.logging.hyperparams['input_filepath'],\n",
    "                                 no_of_examples = clf.logging.hyperparams['no_of_examples'],\n",
    "                                 train_test_split = clf.logging.hyperparams['train_test_split'],\n",
    "                                 train_val_split = clf.logging.hyperparams['train_val_split'])\n",
    "                \n",
    "# Check how the examples are distributed across the classes.\n",
    "class_distribution = clf.datahandler.check_class_distribution(clf.task)\n",
    "clf.plot_class_distribution()\n",
    "clf.plot_random(no_of_spectra = 10, dataset = 'train')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqJ1CeG1WxFM"
   },
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135557,
     "status": "ok",
     "timestamp": 1613580049677,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "JvzitnVNWxFR",
    "outputId": "850d33ad-29df-4da9-bcdb-16d15ba223bc"
   },
   "outputs": [],
   "source": [
    "clf.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51UVlZILWxFW"
   },
   "source": [
    "### Compile and summarize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 137174,
     "status": "ok",
     "timestamp": 1613580051896,
     "user": {
      "displayName": "Lukas Pielsticker",
      "photoUrl": "",
      "userId": "09713923278611487525"
     },
     "user_tz": -60
    },
    "id": "KN24emZEWxFW",
    "outputId": "a4912feb-d2b2-4004-84a6-d260ec4f32dd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError, CategoricalCrossentropy\n",
    "\n",
    "learning_rate = 1e-05\n",
    "optimizer = Adam(learning_rate = learning_rate) \n",
    "\n",
    "if clf.task == 'regression':\n",
    "    mae = MeanAbsoluteError()\n",
    "    clf.model.compile(loss = mae, optimizer = optimizer)\n",
    "    # =============================================================================\n",
    "    # mse = MeanSquaredError()\n",
    "    # clf.model.compile(loss = mse, optimizer = optimizer)\n",
    "    # =============================================================================\n",
    "    \n",
    "if clf.task == 'classification':\n",
    "    categorical_crossentropy = CategoricalCrossentropy()\n",
    "    clf.model.compile(loss = categorical_crossentropy,\n",
    "                      optimizer = optimizer,\n",
    "                      metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# Plot summary and save model plot.\n",
    "clf.summary()\n",
    "clf.save_and_print_model_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr2aMkdaqg2K"
   },
   "source": [
    "### Show predictions with current model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXWyQ_cHqIJ2"
   },
   "outputs": [],
   "source": [
    "pred_train_intermediate, pred_test_intermediate = clf.predict()\n",
    "\n",
    "print(y_train[:5])\n",
    "print(pred_train_intermediate[:5])\n",
    "\n",
    "print(y_test[:5])\n",
    "print(pred_test_intermediate[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcYkliU1WxFa"
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XVQWFw1yWxFa",
    "outputId": "4f3531b3-43a6-4bf7-eead-d9e341416557"
   },
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "#new_learning_rate = 5e-06\n",
    "\n",
    "hist = clf.train(checkpoint = True,\n",
    "                 early_stopping = False,\n",
    "                 tb_log = True, \n",
    "                 csv_log = True,\n",
    "                 epochs = epochs, \n",
    "                 batch_size = batch_size,\n",
    "                 verbose = 1,)\n",
    "#                 new_learning_rate = new_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlkNFbYNWxFe"
   },
   "source": [
    "### Plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9txM4QFyWxFe"
   },
   "outputs": [],
   "source": [
    "dir_name = clf.time + '_' + clf.data_name\n",
    "graph = clfutils.TrainingGraphs(clf.history, dir_name)\n",
    "graph.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OY7AxjWsWxFg"
   },
   "source": [
    "### Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4g_QmUYG5fuT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if clf.task == 'classification':\n",
    "    score = clf.evaluate()\n",
    "    test_loss, test_accuracy = score[0], score[1]\n",
    "    print('Test loss: ' + str(np.round(test_loss, decimals=8)))\n",
    "    print('Test accuracy: ' + str(np.round(test_accuracy, decimals=3)))\n",
    "elif clf.task == 'regression':\n",
    "    test_loss = clf.evaluate()\n",
    "    print('Test loss: ' + str(np.round(test_loss, decimals=8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uCoVCI-LwGY"
   },
   "source": [
    "###  Predict on train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ezklX5YY5fyr"
   },
   "outputs": [],
   "source": [
    "pred_train, pred_test = clf.predict()\n",
    "if clf.task == 'classification':\n",
    "    pred_train_classes, pred_test_classes = clf.predict_classes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyWjEwJoWxFk"
   },
   "source": [
    "### Show some predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV7aAEMQWxFl"
   },
   "source": [
    "#### 10 random training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T6BgD25qWxFl"
   },
   "outputs": [],
   "source": [
    "clf.plot_random(no_of_spectra = 10, dataset = 'train', with_prediction = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18CPy-pqWxFo"
   },
   "source": [
    "#### 10 random test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mlad8aaEWxFo"
   },
   "outputs": [],
   "source": [
    "clf.plot_random(no_of_spectra = 10, dataset = 'test', with_prediction = True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1u-ocIdb1_1Z"
   },
   "source": [
    "### Show wrong/worst predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if clf.task == 'classification':\n",
    "    clf.show_wrong_classification()\n",
    "elif clf.task == 'regression':\n",
    "    clf.show_worst_predictions(no_of_spectra = 20)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PL3Jn60BWxFq"
   },
   "source": [
    "### Save model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Lt9sk16WxFr"
   },
   "outputs": [],
   "source": [
    "#clf.save_model()\n",
    "clf.shelve_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK9TOFCdWxFt"
   },
   "source": [
    "### Generate report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKra_-rCWxFt"
   },
   "outputs": [],
   "source": [
    "dir_name = clf.time + '_' + clf.data_name\n",
    "rep = clfutils.Report(dir_name)  \n",
    "rep.write()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pk-dt26OLwHE"
   },
   "source": [
    "## Save output of notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mkSdyElDLwHF"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Javascript, display\n",
    "from nbconvert import HTMLExporter\n",
    "\n",
    "def save_notebook():\n",
    "    display(Javascript(\"IPython.notebook.save_notebook()\"),\n",
    "            include=['application/javascript'])\n",
    "\n",
    "def output_HTML(read_file, output_file):\n",
    "    import codecs\n",
    "    import nbformat\n",
    "    exporter = HTMLExporter()\n",
    "    # read_file is '.ipynb', output_file is '.html'\n",
    "    output_notebook = nbformat.read(read_file, as_version=4)\n",
    "    output, resources = exporter.from_notebook_node(output_notebook)\n",
    "    codecs.open(output_file, 'w', encoding='utf-8').write(output)\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "time.sleep(20)\n",
    "save_notebook()\n",
    "print('Notebook saved!')\n",
    "time.sleep(30)\n",
    "current_file = '/content/drive/My Drive/app/xpsdeeplearning/train.ipynb'\n",
    "output_file = os.path.join(clf.logging.log_dir,'train_out.html')\n",
    "output_HTML(current_file, output_file)\n",
    "print('HTML file saved!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_multiple.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
