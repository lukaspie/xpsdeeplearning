{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train_prob.ipynb","provenance":[],"collapsed_sections":["0FDlB_v8VaAR"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"vt5hanGL4WpD"},"source":["# Train a deep CNN on XPS data on Google Colab"]},{"cell_type":"markdown","metadata":{"id":"uTjEnuR-LwEo"},"source":["In this notebook, we will train a deep convolutional network on iron XPS spectra made up of linear combinations of single iron reference spectra."]},{"cell_type":"markdown","metadata":{"id":"u6EVGrGFLwEr"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"dU2aEkqdLwE_"},"source":["### Mount google drive, change working directory"]},{"cell_type":"code","metadata":{"id":"5iL11_yXLwFB"},"source":["# Mount drive\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","\n","# Change working path\n","os.chdir('/content/drive/My Drive/deepxps')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rC3yXYXaLwEt"},"source":["### Install packages and import modules"]},{"cell_type":"code","metadata":{"id":"s4MxYB_b33V9"},"source":["%%capture\n","# Install packages\n","!pip install python-docx\n","\n","# Import standard modules and magic commands\n","import datetime\n","import numpy as np\n","import pytz\n","import importlib\n","\n","# Set random seed for reproducible loading\n","np.random.seed(502)\n","\n","# Magic commands\n","%matplotlib inline\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# Disable tf warnings\n","os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n","\n","import tensorflow as tf\n","import tensorflow_probability as tfp\n","tfd = tfp.distributions\n","tf.keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ad6lrbtPUwVk"},"source":["### Check TensorFlow and TensorFlow Probability versions"]},{"cell_type":"code","metadata":{"id":"1wemeM47_Dsy"},"source":["f\"TF version: {tf.__version__}.\"\n","f\"TFP version: {tfp.__version__}.\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMuIKSLjUeBO"},"source":["### Check hardware"]},{"cell_type":"code","metadata":{"id":"2wcWtXa1UcSP"},"source":["from tensorflow.python.profiler import profiler_client\n","\n","if tf.test.gpu_device_name():\n","    print(\"Found GPU: {}\".format(tf.test.gpu_device_name()))\n","else:\n","    print(\"Found no GPU.\")\n","try:\n","    tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\n","    print(\"Found TPU: {}\".format(profiler_client.monitor(tpu_profile_service_address, 100, 2)))\n","except:\n","    print(\"Found no TPU.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R1BPxWcLLwFp"},"source":["## Initial training"]},{"cell_type":"markdown","metadata":{"id":"-qQx0QfuLwFQ"},"source":["### Load custom modules"]},{"cell_type":"code","metadata":{"id":"SfLJzbL04VZ8"},"source":["try:\n","    importlib.reload(classifier)\n","    importlib.reload(clfutils)\n","    print(\"Modules were reloaded.\")\n","except:\n","    import xpsdeeplearning.network.classifier as classifier\n","    import xpsdeeplearning.network.utils as clfutils\n","    print(\"Modules were loaded.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7M70DZczykg"},"source":["### Set up the parameters & folder structure\n","\n"]},{"cell_type":"code","metadata":{"id":"GXy2WdXYXcXc"},"source":["time = datetime.datetime.now().astimezone(pytz.timezone('Europe/Berlin')).strftime(\"%Y%m%d_%Hh%Mm\")\n","exp_name = \"Ni_2_classes_long_linear_comb_small_gas_phase_regression_CNN_bayesian\"\n","#exp_name = \"MNIST_bayesian_classification\"\n","#exp_name = \"shrunken_babys_bayesian_regression\"\n","\n","clf = classifier.Classifier(time=time,\n","                            exp_name=exp_name,\n","                            task=\"classification\",\n","                            intensity_only=True)\n","\n","### If labels not saved with data ###\n","# =============================================================================\n","# labels = ['Fe metal', 'FeO', 'Fe3O4', 'Fe2O3']\n","# clf = classifier.Classifier(time=time,\n","#                            exp_name=exp_name,\n","#                            task='regression',\n","#                            intensity_only=True,\n","#                            labels=labels)\n","# ============================================================================="],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0wkRVOuLwFy"},"source":["### Load and inspect the data"]},{"cell_type":"code","metadata":{"id":"KZIiyOyKtxmf"},"source":["input_filepath = r'/content/drive/My Drive/deepxps/datasets/20210528_Ni_linear_combination_small_gas_phase.h5'\n","\n","train_test_split = 0.2\n","train_val_split = 0.2\n","no_of_examples = 1000#20#00000\n","\n","X_train, X_val, X_test, y_train, y_val, y_test,\\\n","    aug_values_train, aug_values_val, aug_values_test =\\\n","        clf.load_data_preprocess(input_filepath=input_filepath,\n","                                 no_of_examples=no_of_examples,\n","                                 train_test_split=train_test_split,\n","                                 train_val_split=train_val_split)\n","               \n","# Check how the examples are distributed across the classes.\n","class_distribution = clf.datahandler.check_class_distribution(clf.task)\n","clf.plot_class_distribution()\n","clf.plot_random(no_of_spectra=10, dataset='train')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUuFaVy7GFes"},"source":["### Only use classification data\n","if clf.task == \"classification\":\n","    indices_train = np.where(clf.datahandler.y_train == 0.0)[0]\n","    indices_val = np.where(clf.datahandler.y_val == 0.0)[0]\n","    indices_test= np.where(clf.datahandler.y_test == 0.0)[0]\n","\n","    X_train, y_train = clf.datahandler.X_train[indices_train], clf.datahandler.y_train[indices_train]\n","    X_val, y_val = clf.datahandler.X_val[indices_val], clf.datahandler.y_val[indices_val]\n","    X_test, y_test = clf.datahandler.X_test[indices_test], clf.datahandler.y_test[indices_test]\n","\n","    clf.datahandler.X_train, clf.datahandler.y_train = X_train, y_train\n","    clf.datahandler.X_val, clf.datahandler.y_val = X_val, y_val \n","    clf.datahandler.X_test, clf.datahandler.y_test = X_test, y_test\n","\n","    #clf.datahandler.y_train[0:5]\n","    #clf.datahandler.y_val[0:5]\n","    #clf.datahandler.y_test[0:5]\n","\n","    num_train = 250\n","    num_val = 50\n","    num_test = 50\n","\n","    clf.datahandler.X_train, clf.datahandler.y_train = clf.datahandler.X_train[0:num_train], clf.datahandler.y_train[0:num_train]\n","    clf.datahandler.X_val, clf.datahandler.y_val = clf.datahandler.X_train[0:num_val], clf.datahandler.y_train[0:num_val]\n","    clf.datahandler.X_test, clf.datahandler.y_test = clf.datahandler.X_train[0:num_test], clf.datahandler.y_train[0:num_test]\n","    clf.plot_random(no_of_spectra = 10, dataset = 'train')  \n","\n","    print(f\"Remaining no. of training examples: {clf.datahandler.y_train.shape[0]}\")\n","    print(f\"Remaining no. of val examples: {clf.datahandler.y_val.shape[0]}\")\n","    print(f\"Remaining no. of test examples: {clf.datahandler.y_test.shape[0]}\")\n","\n","elif clf.task == \"regression\":\n","    print(\"Dataset was not changed.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FLDm3qpXafNh"},"source":["### Loads MNIST dataset.###\n","import matplotlib.pyplot as plt\n","num_classes = 10\n","\n","batch_size = 128\n","val_split = 0.2\n","\n","\n","print('Loading MNIST dataset')\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","X_train = np.expand_dims(X_train, -1)[:3000]\n","X_test = np.expand_dims(X_test, -1)[:500]\n","\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes)[:3000]\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes)[:500]\n","\n","# Normalize data\n","X_train = X_train.astype('float32') / 255\n","X_test = X_test.astype('float32') / 255\n","\n","# Train-val split\n","num_train = int((1-val_split)*X_train.shape[0])\n","(X_train, X_val) = X_train[:num_train], X_train[num_train:]\n","(y_train, y_val) = y_train[:num_train], y_train[num_train:]\n","\n","\n","clf.datahandler.X_train, clf.datahandler.y_train = X_train, y_train\n","clf.datahandler.X_val, clf.datahandler.y_val = X_val, y_val\n","clf.datahandler.X_test, clf.datahandler.y_test = X_test, y_test\n","clf.datahandler.input_shape =  (clf.datahandler.X_train.shape[1:])\n","clf.datahandler.num_classes = num_classes\n","clf.datahandler.labels = list(range(num_classes))\n","\n","print(\"X_train.shape =\", X_train.shape)\n","print(\"y_train.shape =\", y_train.shape)\n","print(\"X_val.shape =\", X_val.shape)\n","print(\"y_val.shape =\", y_val.shape)\n","print(\"X_test.shape =\", X_test.shape)\n","print(\"y_test.shape =\", y_test.shape)\n","\n","plt.imshow(X_train[0, :, :, 0], cmap='gist_gray')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGZkbWRCIGUq"},"source":["## Loads shrunken baby dataset\n","from PIL import Image\n","import glob\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","val_split = 0.1\n","test_split = 0.1\n","no_of_examples = 100#0\n","\n","image_paths = glob.glob(r\"/content/drive/My Drive/deepxps/datasets/shrunken_baby_ds/*.png\")\n","\n","X = [np.array(Image.open(im)) for im in image_paths]\n","X = [np.expand_dims(image, -1) for image in X] # add extra dimension to each image (126,126) --> (126,126,1)\n","X = np.array(X)[:no_of_examples] # convert list of images to single array [(126,126,1)] --> (836, 126, 126, 1)\n","\n","from skimage.measure import block_reduce\n","\n","X = X[:,5:-21,13:-13,:]\n","new_X = []\n","for image in X:\n","    reduced_image = block_reduce(image,\n","                                 block_size=(3, 3, 1),\n","                                 func=np.mean)\n","    new_X.append(reduced_image)\n","X = np.array(new_X)\n","X /= np.max(X, axis=1)\n","\n","y = pd.read_csv(r\"/content/drive/My Drive/deepxps/datasets/shrunken_baby_ds/shrunken_baby_labels.csv\").to_numpy()[:no_of_examples]\n","\n","# Normalize data\n","#X = X.astype('float32') / 255\n","#y = y.astype('float32')/np.max(y)\n","\n","# Train-test split\n","num_train_val = int((1-test_split)*X.shape[0])\n","(X_train_val, X_test) = X[:num_train_val], X[num_train_val:]\n","(y_train_val, y_test) = y[:num_train_val], y[num_train_val:]\n","\n","# Train-val split\n","num_train = int((1-val_split)*X_train_val.shape[0])\n","(X_train, X_val) = X_train_val[:num_train], X_train_val[num_train:]\n","(y_train, y_val) = y_train_val[:num_train], y_train_val[num_train:]\n","\n","clf.datahandler.X, clf.datahandler.y = X, y\n","clf.datahandler.X_train, clf.datahandler.y_train = X_train, y_train\n","clf.datahandler.X_val, clf.datahandler.y_val = X_val, y_val\n","clf.datahandler.X_test, clf.datahandler.y_test = X_test, y_test\n","clf.datahandler.input_shape =  (clf.datahandler.X_train.shape[1:])\n","clf.datahandler.num_classes = 1\n","clf.datahandler.labels = [\"sizes\"]\n","\n","print(\"No. of examples: \", X.shape)\n","print(\"X_train.shape =\", X_train.shape)\n","print(\"y_train.shape =\", y_train.shape)\n","print(\"X_val.shape =\", X_val.shape)\n","print(\"y_val.shape =\", y_val.shape)\n","print(\"X_test.shape =\", X_test.shape)\n","print(\"y_test.shape =\", y_test.shape)\n","\n","for i in range(10):\n","    r = np.random.randint(0,X.shape[0])\n","    plt.imshow(np.squeeze(X[r]))\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TKXmi1LMLwF6"},"source":["### Design the model"]},{"cell_type":"code","metadata":{"id":"pyxJ9_qh5awz"},"source":["try:\n","    importlib.reload(models)\n","    print(\"Models module was reloaded.\")\n","except:\n","    import xpsdeeplearning.network.models as models\n","    print(\"Models module was loaded.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_BdxwGerb2ZQ"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","from tensorflow_probability.python.layers import util as tfp_layers_util\n","\n","class BayesianCNN(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at \n","    the beginning. Works well for learning across scales.\n","    \n","    This is to be used for regression on all labels. -> sigmoid \n","    activation in the last layer.\n","    \"\"\"\n","    def __init__(\n","        self, \n","        inputshape,\n","        num_classes,\n","        kl_divergence_fn,\n","        task,\n","        ):\n","        if len(inputshape) == 2:\n","            conv_layer = tfp.layers.Convolution1DFlipout\n","            strides = 1\n","            average_pool_layer = layers.AveragePooling1D\n","\n","        elif len(inputshape) == 3:\n","            conv_layer = tfp.layers.Convolution2DFlipout\n","            strides = (1,1)\n","            average_pool_layer =  layers.AveragePooling2D\n","\n","        if task == \"regression\":\n","            if num_classes == 1:\n","                output_act = None\n","            else:\n","                output_act = \"sigmoid\"\n","        elif task == \"classification\":\n","            output_act = \"softmax\"\n","\n","        self.input_1 = layers.Input(shape = inputshape)\n","                \n","        self.conv_1_short = conv_layer(\n","            filters=12,\n","            kernel_size=5,\n","            strides=strides,\n","            padding='same',\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='conv_1_short')(self.input_1)\n","        self.conv_1_medium = conv_layer(\n","            filters=12,\n","            kernel_size=10,\n","            strides=strides,\n","            padding='same',\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='conv_1_medium')(self.input_1)\n","        self.conv_1_long = conv_layer(\n","            filters=12,\n","            kernel_size=15,\n","            strides=strides,\n","            padding='same',\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='conv_1_long')(self.input_1)\n","        \n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding='valid',\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='conv_2')(merged_sublayers)\n","        self.conv_3 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding='valid',\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = average_pool_layer(\n","            name='average_pool_1')(self.conv_3)\n","        \n","        self.flatten_1 = layers.Flatten(name='flatten1')(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(rate=0.2,\n","                                     name='drop_1')(self.flatten_1)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=4000,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='dense_1')(self.flatten_1)\n","                           \n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation=output_act,\n","            name='dense_2')(self.dense_1)\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(BayesianCNN, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.dense_2,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name='BayesianCNN')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0FDlB_v8VaAR"},"source":["#### Not used"]},{"cell_type":"code","metadata":{"id":"mtDGsyAbzxm0"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","\n","tf.keras.backend.clear_session()\n","class ProbabilityCNNTest2D(models.EmptyModel):\n","    \"\"\"\n","    \n","    This is to be used for regression on all labels. -> sigmoid \n","    activation in the last layer.\n","    \"\"\"\n","    def __init__(self, inputshape, num_classes, kl_divergence_function):   \n","        self.input_1 = tf.keras.Input(shape = inputshape)\n","        self.conv_1 = tfp.layers.Convolution2DFlipout(\n","            filters=16,\n","            kernel_size=5,\n","            strides=(1,1),\n","            padding=\"same\", \n","            activation=\"relu\",\n","            name=\"conv_1\", \n","            kernel_divergence_fn=kl_divergence_function)(self.input_1)    \n","        self.mp_1 = layers.MaxPool2D(\n","            strides=(4,4), \n","            pool_size=(4,4), \n","            padding=\"same\")(self.conv_1)\n","        self.conv_2 = tfp.layers.Convolution2DFlipout(\n","            filters=32,\n","            kernel_size=3, \n","            strides=(1,1),\n","            padding=\"same\",\n","            activation=\"relu\", \n","            name=\"conv_2\",\n","            kernel_divergence_fn=kl_divergence_function)(self.mp_1)\n","        self.mp_2 = layers.MaxPool2D(\n","            strides=(4,4), \n","            pool_size=(4,4), \n","            padding=\"same\")(self.conv_2)\n","        self.flatten = layers.Flatten()(self.mp_2)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_function)(self.flatten)\n","        no_of_inputs = 1\n","\n","        super(ProbabilityCNNTest2D, self).__init__(inputs=self.input_1,\n","                                                 outputs=self.dense_1,\n","                                                 inputshape=inputshape,\n","                                                 num_classes=num_classes,\n","                                                 no_of_inputs=no_of_inputs,\n","                                                 name='ProbabilityCNNTest2D')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PejxNbsJThXJ"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","\n","class ProbabilisticClassificationCNN(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at \n","    the beginning. Works well for learning across scales.\n","    \n","    This is to be used for classification -> softmax activation in the\n","    last layer.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 inputshape, \n","                 num_classes,\n","                 kl_divergence_function):\n","\n","        self.input_1 = layers.Input(shape=inputshape)\n","\n","        self.conv_1_short = tfp.layers.Convolution1DFlipout(\n","            filters=4,\n","            kernel_size=5,\n","            strides=1,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function,\n","            activation=\"relu\",\n","            name=\"conv_1_short\",\n","        )(self.input_1)\n","        self.conv_1_medium = tfp.layers.Convolution1DFlipout(\n","            filters=4,\n","            kernel_size=10,\n","            strides=1,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function,\n","            activation=\"relu\",\n","            name=\"conv_1_medium\",\n","        )(self.input_1)\n","        self.conv_1_long = tfp.layers.Convolution1DFlipout(\n","            filters=4,\n","            kernel_size=15,\n","            strides=1,\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_long\",\n","        )(self.input_1)\n","\n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = tfp.layers.Convolution1DFlipout(\n","            filters=4,\n","            kernel_size=10,\n","            strides=1,\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function,\n","            padding=\"valid\",\n","            activation=\"relu\",\n","            name=\"conv_2\",\n","        )(merged_sublayers)\n","        self.conv_3 = tfp.layers.Convolution1DFlipout(\n","            filters=10,\n","            kernel_size=10,\n","            strides=1,\n","            padding=\"valid\",\n","            activation=\"relu\",\n","            name=\"conv_3\",\n","        )(self.conv_2)\n","        self.average_pool_1 = layers.AveragePooling1D(name=\"average_pool_1\")(\n","            self.conv_3\n","        )\n","        self.flatten_1 = layers.Flatten(name=\"flatten1\")(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(rate=0.2, name=\"drop_1\")(self.flatten_1)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=1000,\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function, \n","            activation=\"relu\",\n","            name=\"dense_1\"\n","        )(self.drop_1)\n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function, \n","            activation=\"softmax\", \n","            name=\"dense_2\"\n","        )(self.dense_1)\n","        #self.dist_outputs = tfp.layers.OneHotCategorical(\n","        #    event_size=num_classes,\n","        #    convert_to_tensor_fn=tfp.distributions.Distribution.sample,\n","        #    )(self.dense_2)\n","        \n","        no_of_inputs = len(sublayers)\n","\n","        super(ProbabilisticClassificationCNN, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.dense_2,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"ProbabilisticClassificationCNN\",\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SxhSCc1hax8-"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","\n","class ProbabilisticClassificationCNN2D(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at \n","    the beginning. Works well for learning across scales.\n","    \n","    This is to be used for classification -> softmax activation in the\n","    last layer.\n","    \n","    2D model for e.g. MNIST.\n","    Implements Bayes by Backprop.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 inputshape, \n","                 num_classes,\n","                 kl_divergence_function):\n","\n","        self.input_1 = layers.Input(shape=inputshape)\n","\n","        self.conv_1_short = tfp.layers.Convolution2DFlipout(\n","            filters=4,\n","            kernel_size=5,\n","            strides=1,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function,\n","            activation=\"relu\",\n","            name=\"conv_1_short\",\n","        )(self.input_1)\n","        self.conv_1_medium = tfp.layers.Convolution2DFlipout(\n","            filters=4,\n","            kernel_size=10,\n","            padding=\"same\",\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function,\n","            activation=\"relu\",\n","            name=\"conv_1_medium\",\n","        )(self.input_1)\n","        self.conv_1_long = tfp.layers.Convolution2DFlipout(\n","            filters=4,\n","            kernel_size=15,\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function,\n","            padding=\"same\",\n","            activation=\"relu\",\n","            name=\"conv_1_long\",\n","        )(self.input_1)\n","\n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = tfp.layers.Convolution2DFlipout(\n","            filters=4,\n","            kernel_size=5,\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function,\n","            padding=\"valid\",\n","            activation=\"relu\",\n","            name=\"conv_2\",\n","        )(merged_sublayers)\n","\n","        self.average_pool_1 = layers.AveragePooling2D(name=\"average_pool_1\")(\n","            self.conv_2\n","        )\n","\n","        self.flatten_1 = layers.Flatten(name=\"flatten1\")(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(rate=0.2, name=\"drop_1\")(self.flatten_1)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=1000,\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function, \n","            activation=\"relu\",\n","            name=\"dense_1\"\n","        )(self.drop_1)\n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_function,\n","            bias_divergence_fn=kl_divergence_function, \n","            activation=\"softmax\", \n","            name=\"dense_2\"\n","        )(self.dense_1)\n","        \n","        no_of_inputs = len(sublayers)\n","\n","        super(ProbabilisticClassificationCNN2D, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.dense_2,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name=\"ProbabilisticClassificationCNN2D\",\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zlav04xq9Xr6"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","\n","class ProbabilisticCNN2D(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at \n","    the beginning. Works well for learning across scales.\n","    \n","    This is to be used for regression on all labels. -> sigmoid \n","    activation in the last layer.\n","    \"\"\"\n","    def __init__(self, \n","                 inputshape,\n","                 num_classes,\n","                 kl_divergence_function,\n","                 ):\n","        self.input_1 = layers.Input(shape = inputshape)\n","                \n","        self.conv_1_short = tfp.layers.Convolution2DFlipout(\n","            filters=12,\n","            kernel_size=5,\n","            strides=(1,1),\n","            padding='same',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='conv_1_short')(self.input_1)\n","        self.conv_1_medium = tfp.layers.Convolution2DFlipout(\n","            filters=12,\n","            kernel_size=10,\n","            strides=(1,1),\n","            padding='same',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='conv_1_medium')(self.input_1)\n","        self.conv_1_long = tfp.layers.Convolution2DFlipout(\n","            filters=12,\n","            kernel_size=15,\n","            strides=(1,1),\n","            padding='same',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='conv_1_long')(self.input_1)\n","        \n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = tfp.layers.Convolution2DFlipout(\n","            filters=10,\n","            kernel_size=5,\n","            strides=(1,1),\n","            padding='valid',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='conv_2')(merged_sublayers)\n","        self.conv_3 = tfp.layers.Convolution2DFlipout(\n","            filters=10,\n","            kernel_size=5,\n","            strides=(1,1),\n","            padding='valid',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = layers.AveragePooling2D(\n","            name='average_pool_1')(self.conv_3)\n","        \n","        self.flatten_1 = layers.Flatten(name='flatten1')(self.average_pool_1)\n","        #self.drop_1 = layers.Dropout(rate=0.2,\n","        #                             name='drop_1')(self.flatten_1)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=4000,\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='dense_1')(self.flatten_1)\n","                           \n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation=None,#\"sigmoid\",\n","            name='dense_2')(self.dense_1)\n","        \n","        #self.output_norm = layers.Lambda(\n","        #    lambda x: x/tf.reshape(K.sum(x, axis=-1),(-1,1)),\n","        #    name = 'output_normalization')(self.dense_2)\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(ProbabilisticCNN2D, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.dense_2,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name='ProbabilisticCNN2D')\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"olHiSebSpftz"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","\n","class ProbabilisticCNNNoAct2D(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at \n","    the beginning. Works well for learning across scales.\n","    \n","    This is to be used for regression on all labels. -> sigmoid \n","    activation in the last layer.\n","    \"\"\"\n","    def __init__(self, \n","                 inputshape,\n","                 num_classes,\n","                 kl_divergence_function,\n","                 ):\n","        self.input_1 = layers.Input(shape = inputshape)\n","                \n","        self.conv_1_short = tfp.layers.Convolution2DFlipout(\n","            filters=12,\n","            kernel_size=5,\n","            strides=(1,1),\n","            padding='same',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='conv_1_short')(self.input_1)\n","        self.conv_1_medium = tfp.layers.Convolution2DFlipout(\n","            filters=12,\n","            kernel_size=10,\n","            strides=(1,1),\n","            padding='same',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='conv_1_medium')(self.input_1)\n","        self.conv_1_long = tfp.layers.Convolution2DFlipout(\n","            filters=12,\n","            kernel_size=15,\n","            strides=(1,1),\n","            padding='same',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='conv_1_long')(self.input_1)\n","        \n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = tfp.layers.Convolution2DFlipout(\n","            filters=10,\n","            kernel_size=5,\n","            strides=(1,1),\n","            padding='valid',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='conv_2')(merged_sublayers)\n","        self.conv_3 = tfp.layers.Convolution2DFlipout(\n","            filters=10,\n","            kernel_size=5,\n","            strides=(1,1),\n","            padding='valid',\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = layers.AveragePooling2D(\n","            name='average_pool_1')(self.conv_3)\n","        \n","        self.flatten_1 = layers.Flatten(name='flatten1')(self.average_pool_1)\n","        #self.drop_1 = layers.Dropout(rate=0.2,\n","        #                             name='drop_1')(self.flatten_1)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=4000,\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation='relu',\n","            name='dense_1')(self.flatten_1)\n","                           \n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_function,\n","            activation=None, # no activation in output layer\n","            name='dense_2')(self.dense_1)\n","        \n","        #self.output_norm = layers.Lambda(\n","        #    lambda x: x/tf.reshape(K.sum(x, axis=-1),(-1,1)),\n","        #    name = 'output_normalization')(self.dense_2)\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(ProbabilisticCNNNoAct2D, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.dense_2,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name='ProbabilisticCNNNoAct2D')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g4bg68GdpzOJ"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","from tensorflow_probability.python.layers import util as tfp_layers_util\n","\n","class BayesianCNN(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at \n","    the beginning. Works well for learning across scales.\n","    \n","    This is to be used for regression on all labels. -> sigmoid \n","    activation in the last layer.\n","    \"\"\"\n","    def __init__(\n","        self, \n","        inputshape,\n","        num_classes,\n","        kl_divergence_fn,\n","        kernel_prior_fn=tfp.layers.default_multivariate_normal_fn,\n","        kernel_posterior_fn=tfp_layers_util.default_mean_field_normal_fn(),\n","        ):\n","        if len(inputshape) == 2:\n","            conv_layer = tfp.layers.Convolution1DFlipout\n","            strides = 1\n","            average_pool_layer = layers.AveragePooling1D\n","\n","        elif len(inputshape) == 3:\n","            conv_layer = tfp.layers.Convolution2DFlipout\n","            strides = (1,1)\n","            average_pool_layer =  layers.AveragePooling2D\n","\n","        self.input_1 = layers.Input(shape = inputshape)\n","                \n","        self.conv_1_short = conv_layer(\n","            filters=12,\n","            kernel_size=5,\n","            strides=strides,\n","            padding='same',\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_posterior_fn=kernel_posterior_fn,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='conv_1_short')(self.input_1)\n","        self.conv_1_medium = conv_layer(\n","            filters=12,\n","            kernel_size=10,\n","            strides=strides,\n","            padding='same',\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_posterior_fn=kernel_posterior_fn,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='conv_1_medium')(self.input_1)\n","        self.conv_1_long = conv_layer(\n","            filters=12,\n","            kernel_size=15,\n","            strides=strides,\n","            padding='same',\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_posterior_fn=kernel_posterior_fn,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='conv_1_long')(self.input_1)\n","        \n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding='valid',\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_posterior_fn=kernel_posterior_fn,            \n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='conv_2')(merged_sublayers)\n","        self.conv_3 = conv_layer(\n","            filters=10,\n","            kernel_size=5,\n","            strides=strides,\n","            padding='valid',\n","            kernel_prior_fn=kernel_prior_fn,\n","            kernel_posterior_fn=kernel_posterior_fn,            \n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = average_pool_layer(\n","            name='average_pool_1')(self.conv_3)\n","        \n","        self.flatten_1 = layers.Flatten(name='flatten1')(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(rate=0.2,\n","                                     name='drop_1')(self.flatten_1)\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=4000,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation='relu',\n","            name='dense_1')(self.drop_1)\n","                           \n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            kernel_divergence_fn=kl_divergence_fn,\n","            activation=\"softmax\", # no activation in output layer\n","            name='dense_2')(self.dense_1)\n","        #self.outputs = tfp.layers.IndependentNormal(\n","        #    event_shape=num_classes,\n","        #    name = \"output_dist\")(self.dense_2)\n","\n","        #self.output_norm = layers.Lambda(\n","        #    lambda x: x/tf.reshape(K.sum(x, axis=-1),(-1,1)),\n","        #    name = 'output_normalization')(self.dense_2)\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(BayesianCNN, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.dense_2,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name='BayesianCNN')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9X0ydiIuf4sn"},"source":["from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","from tensorflow_probability.python.layers import util as tfp_layers_util\n","\n","class BayesianCNN2(models.EmptyModel):\n","    def __init__(\n","        self, \n","        inputshape,\n","        num_classes,\n","        kl_divergence_fn,\n","        ):\n","        if len(inputshape) == 2:\n","            conv_layer = tfp.layers.Convolution1DFlipout\n","            global_avg_pool_layer = layers.GlobalAveragePooling1D\n","\n","        elif len(inputshape) == 3:\n","            conv_layer = tfp.layers.Convolution2DFlipout\n","            average_pool_layer =  layers.GlobalAveragePooling2D      \n","      \n","        self.input_1 = layers.Input(shape = inputshape)\n","                \n","        self.conv_1 = conv_layer(\n","            filters=12,\n","            kernel_size=3,\n","            activation = \"relu\",\n","            kernel_divergence_fn = kernel_divergence_fn,\n","            name=\"conv_1\")(self.input_1)\n","\n","        self.conv_2 = conv_layer(\n","            filters=24,\n","            kernel_size=5,\n","            activation = \"relu\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            name='conv_2')(self.conv_1)\n","\n","        self.conv_3 = conv_layer(\n","            filters=48,\n","            kernel_size=7,\n","            activation = \"relu\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            name='conv_3')(self.conv_1)    \n","\n","        self.global_avg_pool_1 = average_pool_layer(\n","            name=\"global_avg_pool_1\"\n","        )(self.conv_3)  \n","\n","        self.dense_1 = tfp.layers.DenseFlipout(\n","            units=48,\n","            activation = \"relu\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            name='dense_1')(self.global_avg_pool_1)  \n","        self.dense_2 = tfp.layers.DenseFlipout(\n","            units=num_classes,\n","            activation = \"softmax\",\n","            kernel_divergence_fn=kl_divergence_fn,\n","            name='dense_2')(self.dense_1)              \n","        #self.outputs = tfp.layers.OneHotCategorical(\n","        #    event_size=num_classes,\n","        #    name=\"output_dist\")(self.dense_2)\n","\n","        no_of_inputs = 1\n","\n","        super(BayesianCNN2, self).__init__(\n","            inputs=self.input_1,\n","            outputs=self.dense_2,\n","            inputshape=inputshape,\n","            num_classes=num_classes,\n","            no_of_inputs=no_of_inputs,\n","            name='BayesianCNN2')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"76OKygMa9Xr9"},"source":["#### Design prior, posterior and KL divergence function"]},{"cell_type":"code","metadata":{"id":"XVts9GTPXueH"},"source":["kl_divergence_fn = (\n","    lambda q, p, _: (tfp.distributions.kl_divergence(q, p)) /\n","    tf.cast(clf.datahandler.X_train.shape[0], dtype=tf.float32)\n","    )\n","\n","# =============================================================================\n","# #kernel_prior_fn=tfp.layers.default_multivariate_normal_fn,\n","# #kernel_posterior_fn=tfp_layers_util.default_mean_field_normal_fn(),\n","# \n","# # Define the prior weight distribution as Normal of mean=0 and stddev=1.\n","# # Note that, in this example, the we prior distribution is not trainable,\n","# # as we fix its parameters.\n","# def kernel_prior_fn(kernel_size, bias_size, dtype=None):\n","#     n = kernel_size + bias_size\n","#     prior_model = keras.Sequential(\n","#         [\n","#             tfp.layers.DistributionLambda(\n","#                 lambda t: tfp.distributions.MultivariateNormalDiag(\n","#                     loc=tf.zeros(n), scale_diag=tf.ones(n)\n","#                 )\n","#             )\n","#         ]\n","#     )\n","#     return prior_model\n","# \n","# \n","# # Define variational posterior weight distribution as multivariate Gaussian.\n","# # Note that the learnable parameters for this distribution are the means,\n","# # variances, and covariances.\n","# def kernel_posterior_fn(kernel_size, bias_size, dtype=None):\n","#     n = kernel_size + bias_size\n","#     posterior_model = keras.Sequential(\n","#         [\n","#             tfp.layers.VariableLayer(\n","#                 tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n","#             ),\n","#             tfp.layers.MultivariateNormalTriL(n),\n","#         ]\n","#     )\n","#     return posterior_model\n","# ============================================================================="],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zHpFBDmgXoMW"},"source":["#### Build the model"]},{"cell_type":"code","metadata":{"id":"Af3wbjEW9Xr-"},"source":["clf.model = BayesianCNN(\n","    inputshape=clf.datahandler.input_shape,\n","    num_classes=clf.datahandler.num_classes,\n","    kl_divergence_fn=kl_divergence_fn,\n","    task=clf.task)\n","#    kernel_prior_fn=kernel_posterior_fn,\n","#    kernel_posterior_fn=kernel_posterior_fn)\n","\n","# =============================================================================\n","# clf.model = ProbabilisticCNN(clf.datahandler.input_shape,\n","#                              clf.datahandler.num_classes,\n","#                              kl_divergence_fn)\n","# \n","# clf.model = ProbabilisticClassificationCNN2D(clf.datahandler.input_shape,\n","#                                              clf.datahandler.num_classes,\n","#                                              kl_divergence_fn)\n","# clf.model = ProbabilisticClassificationMLP(\n","#     inputshape=clf.datahandler.input_shape,\n","#     num_classes=clf.datahandler.num_classes,\n","#     kl_divergence_fn=kl_divergence_fn)\n","# \n","# clf.model = ProbabilisticRegressionMLP(\n","#     inputshape=clf.datahandler.input_shape,\n","#     num_classes=clf.datahandler.num_classes,\n","#     kl_divergence_fn=kl_divergence_fn)\n","# \n","# clf.model = RegressionMLP(\n","#     inputshape=clf.datahandler.input_shape,\n","#     num_classes=clf.datahandler.num_classes)\n","# =============================================================================\n","\n","# Alternative: Build model from available models in models.py\n","# =============================================================================\n","#clf.model = models.RegressionCNN(clf.datahandler.input_shape, \n","#                                 clf.datahandler.num_classes)\n","# =============================================================================\n","# =============================================================================\n","# models.clf.model = ClassificationCNN(clf.datahandler.input_shape,\n","#                              clf.datahandler.num_classes)\n","# =============================================================================\n","\n","# =============================================================================\n","#clf.model = models.ClassificationCNN2D(clf.datahandler.input_shape,\n","#                                       clf.datahandler.num_classes)\n","# =============================================================================\n","\n","# =============================================================================\n","# clf.model = models.ProbabilisticClassificationCNN2D(\n","#     clf.datahandler.input_shape,\n","#     clf.datahandler.num_classes,\n","#     kl_divergence_fn,\n","#     bias_divergence_fn)\n","# =============================================================================\n","\n","# =============================================================================\n","# clf.model = models.ResNet1D(clf.datahandler.input_shape,\n","#                             clf.datahandler.num_classes,\n","#                            ap=True)\n","# =============================================================================\n","# =============================================================================\n","# clf.model = models.ResNet1D(clf.datahandler.input_shape,\n","#                             clf.datahandler.num_classes,\n","#                             ap=True)\n","# =============================================================================\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kZ5JOdivbbJc"},"source":["#### Design loss "]},{"cell_type":"code","metadata":{"id":"tzgwBq24baBe"},"source":["# =============================================================================\n","# def _neg_log_likelihood_bayesian(y_true, y_pred):\n","#     labels_distribution = tfp.distributions.OneHotCategorical(logits=y_pred)\n","#     return -tf.reduce_mean(labels_distribution.log_prob(y_true))\n","# ============================================================================="],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I2SpBC-R9XsA"},"source":["### Compile and summarize the model"]},{"cell_type":"code","metadata":{"id":"vKQZDayn9XsB"},"source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","\n","learning_rate = 1e-04\n","optimizer = Adam(learning_rate = learning_rate) \n","\n","if clf.task == \"regression\":\n","    mae = MeanAbsoluteError()\n","    mse = MeanSquaredError()\n","    clf.model.compile(loss=mse,\n","                      optimizer=optimizer,\n","                      metrics=[\"mse\"])\n","    # =============================================================================\n","    # mse = MeanSquaredError()\n","    # clf.model.compile(loss = mse, optimizer = optimizer)\n","    # =============================================================================\n","    \n","elif clf.task == \"classification\":\n","    categorical_crossentropy = CategoricalCrossentropy()\n","    clf.model.compile(loss=categorical_crossentropy,\n","                      optimizer=optimizer,\n","                      metrics=[\"accuracy\",\n","                               \"categorical_crossentropy\"])\n","\n","# Plot summary and save model plot.\n","clf.summary()\n","clf.save_and_print_model_image()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ukB2YPfgRjyS"},"source":["### Show initial weight distributions"]},{"cell_type":"code","metadata":{"id":"yzktfypsN3ud"},"source":["clf.plot_weight_distribution(kind=\"prior\", to_file=True)\n","clf.plot_weight_distribution(kind=\"posterior\", to_file=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oeboNYkNsf7Z"},"source":["### Show initial predictions"]},{"cell_type":"code","metadata":{"id":"Tpj40ttsCpEj"},"source":["pred_train_initial, pred_test_initial = clf.predict()\n","\n","print('Train:')\n","for i in range(5):\n","    print('real: ' + str(np.round(y_train[i],3)),\n","          'pred: ' + str(pred_train_initial[i]))\n","print('Test:')\n","for i in range(5):\n","    print('real: ' + str(np.round(y_test[i],3)),\n","          'pred: ' + str(pred_test_initial[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RO4qrjPX0pFU"},"source":["no_of_predictions = 100\n","\n","prob_pred_test_initial = clf.predict_probabilistic(\n","    dataset=\"test\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_test_initial[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_test[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDOgVS_bLwGC"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"abV_fFCb5fiZ"},"source":["epochs = 1000\n","batch_size = 32\n","validation_freq = 1\n","\n","hist = clf.train(checkpoint=True,\n","                 early_stopping=False,\n","                 tb_log=True, \n","                 csv_log=True,\n","                 hyperparam_log=True,\n","                 #cb_parameters={\"es_patience\":15,},\n","                 epochs=epochs, \n","                 batch_size=batch_size,\n","                 validation_freq=validation_freq,\n","                 verbose=2)\n","\n","sound = False\n","if sound:\n","    from google.colab import output\n","    output.eval_js('new Audio(\"http://soundbible.com/grab.php?id=1795&type=mp3\").play()')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9uYqsMlLwGJ"},"source":["### Plot loss"]},{"cell_type":"code","metadata":{"id":"Ffr2nfZy5nYi"},"source":["graph = clfutils.TrainingGraphs(clf.logging.history, clf.logging.fig_dir)\n","graph.plot_loss(to_file=True)\n","if clf.task == \"classification\":\n","    graph.plot_accuracy(to_file=True)\n","    graph.plot_metric(\"categorical_crossentropy\",\n","                      to_file=True)\n","if clf.task == \"regression\":\n","    graph.plot_mse(to_file = True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jnAnvSXLLwGQ"},"source":["### Evaluate on test data"]},{"cell_type":"code","metadata":{"id":"4g_QmUYG5fuT","scrolled":true},"source":["if clf.task == 'classification':\n","    score = clf.evaluate()\n","    test_loss, test_accuracy = score[0], score[1]\n","    print('Test loss: ' + str(np.round(test_loss, decimals=8)))\n","    print('Test accuracy: ' + str(np.round(test_accuracy, decimals=3)))\n","elif clf.task == 'regression':\n","    test_loss = clf.evaluate()\n","    print('Test loss: ' + str(np.round(test_loss, decimals=8)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2uCoVCI-LwGY"},"source":["###  Predict on train and test data"]},{"cell_type":"code","metadata":{"id":"ezklX5YY5fyr"},"source":["pred_train, pred_test = clf.predict()\n","if clf.task == 'classification':\n","    pred_train_classes, pred_test_classes = clf.predict_classes()\n","\n","print('Train:')\n","for i in range(5):\n","    print('real: ' + str(np.round(y_train[i],3)),\n","          'pred: ' + str(pred_train[i]))\n","print('Test:')\n","for i in range(5):\n","    print('real: ' + str(np.round(y_test[i],3)),\n","          'pred: ' + str(pred_test[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AYyRZWgcumRJ"},"source":["no_of_predictions = 100\n","\n","prob_pred_test = clf.predict_probabilistic(\n","    dataset=\"test\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_test[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_test[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nWbogghiLwGl"},"source":["### Show some predictions"]},{"cell_type":"markdown","metadata":{"id":"T5DGB_OGLwGm"},"source":["#### 10 random training samples"]},{"cell_type":"code","metadata":{"id":"keDKIJriLwGn"},"source":["clf.plot_random(no_of_spectra=10, dataset='train', with_prediction=True)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-PHAS9XILwGr"},"source":["#### 10 random test samples"]},{"cell_type":"code","metadata":{"id":"HM3ZZf-qLwGs"},"source":["clf.plot_random(no_of_spectra=10, dataset='test', with_prediction=True)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ROK1zo8xzBZF"},"source":["### Show wrong/worst predictions"]},{"cell_type":"code","metadata":{"cellView":"both","id":"uHFC5PWQzE19"},"source":["if clf.task == 'classification':\n","    clf.show_wrong_classification()\n","elif clf.task == 'regression':\n","    clf.show_worst_predictions(no_of_spectra=20)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ii2umXAZuPZI"},"source":["### Show posterior weight distribution after training update"]},{"cell_type":"code","metadata":{"id":"wcvroHH-Tb8x"},"source":["clf.plot_weight_distribution(kind=\"prior\", to_file=True)\n","clf.plot_weight_distribution(kind=\"posterior\", to_file=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wp6sYutg942J"},"source":["### Show distribution of probabilistic predictions"]},{"cell_type":"code","metadata":{"id":"Ez4DC9SUUszH"},"source":["clf.plot_prob_predictions(dataset=\"test\",\n","                          no_of_spectra=10,\n","                          to_file=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KwrgvaLpLwG3"},"source":["### Save model and results"]},{"cell_type":"code","metadata":{"id":"2HBdnZSq5f2D"},"source":["#clf.save_model()\n","clf.pickle_results()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c6B2wITlLwG_"},"source":["### Generate report"]},{"cell_type":"code","metadata":{"id":"-rPXD0ki5pOp"},"source":["dir_name = clf.time + '_' + clf.exp_name\n","rep = clfutils.Report(dir_name)  \n","rep.write()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4mJKWJxWllD"},"source":["## Continue training"]},{"cell_type":"markdown","metadata":{"id":"VH95P7yXcCTq"},"source":["### Load custom modules"]},{"cell_type":"code","metadata":{"id":"ILECvnh6cCTr"},"source":["try:\n","    import importlib\n","    importlib.reload(classifier)\n","    importlib.reload(clfutils)\n","    print(\"\\n Modules were reloaded.\")\n","except:\n","    import xpsdeeplearning.network.classifier as classifier\n","    import xpsdeeplearning.network.utils as clfutils\n","    print(\"Modules were loaded.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qu-oYRDYv93B"},"source":["### Reload classifier from previous run"]},{"cell_type":"code","metadata":{"id":"fffC8Ch1stjm"},"source":["runpath = r\"/content/drive/My Drive/deepxps/runs/20210727_12h29m_MNIST_bayesian_classification\"\n","clf = classifier.restore_clf_from_logs(runpath)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hO_5SSXX0O7I"},"source":["### Load and inspect the data"]},{"cell_type":"code","metadata":{"id":"bXXFcFUCWxFI"},"source":["X_train, X_val, X_test, y_train, y_val, y_test,\\\n","    aug_values_train, aug_values_val, aug_values_test =\\\n","        clf.load_data_preprocess(input_filepath=clf.logging.hyperparams['input_filepath'],\n","                                 no_of_examples=clf.logging.hyperparams['no_of_examples'],\n","                                 train_test_split=clf.logging.hyperparams['train_test_split'],\n","                                 train_val_split=clf.logging.hyperparams['train_val_split'])\n","                \n","# Check how the examples are distributed across the classes.\n","class_distribution = clf.datahandler.check_class_distribution(clf.task)\n","clf.plot_class_distribution()\n","clf.plot_random(no_of_spectra = 10, dataset = 'train')  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oVk1OCP5fg0h"},"source":["### Only use classification data\n","if clf.task == \"classification\":\n","    indices_train = np.where(clf.datahandler.y_train == 0.0)[0]\n","    indices_val = np.where(clf.datahandler.y_val == 0.0)[0]\n","    indices_test= np.where(clf.datahandler.y_test == 0.0)[0]\n","\n","    X_train, y_train = clf.datahandler.X_train[indices_train], clf.datahandler.y_train[indices_train]\n","    X_val, y_val = clf.datahandler.X_val[indices_val], clf.datahandler.y_val[indices_val]\n","    X_test, y_test = clf.datahandler.X_test[indices_test], clf.datahandler.y_test[indices_test]\n","\n","    clf.datahandler.X_train, clf.datahandler.y_train = X_train, y_train\n","    clf.datahandler.X_val, clf.datahandler.y_val = X_val, y_val \n","    clf.datahandler.X_test, clf.datahandler.y_test = X_test, y_test\n","\n","    #clf.datahandler.y_train[0:5]\n","    #clf.datahandler.y_val[0:5]\n","    #clf.datahandler.y_test[0:5]\n","\n","    num_train = 50#0\n","    num_val = 10#0\n","    num_test = 10#0\n","\n","    clf.datahandler.X_train, clf.datahandler.y_train = clf.datahandler.X_train[0:num_train], clf.datahandler.y_train[0:num_train]\n","    clf.datahandler.X_val, clf.datahandler.y_val = clf.datahandler.X_train[0:num_val], clf.datahandler.y_train[0:num_val]\n","    clf.datahandler.X_test, clf.datahandler.y_test = clf.datahandler.X_train[0:num_test], clf.datahandler.y_train[0:num_test]\n","    clf.plot_random(no_of_spectra = 10, dataset = 'train')  \n","\n","    print(f\"Remaining no. of training examples: {clf.datahandler.y_train.shape[0]}\")\n","    print(f\"Remaining no. of val examples: {clf.datahandler.y_val.shape[0]}\")\n","    print(f\"Remaining no. of test examples: {clf.datahandler.y_test.shape[0]}\")\n","\n","elif clf.task == \"regression\":\n","    print(\"Dataset was not changed.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BqJ1CeG1WxFM"},"source":["### Load the model"]},{"cell_type":"code","metadata":{"id":"JvzitnVNWxFR"},"source":["### Currently not working, does not load prior/posterior distributions ####\n","# clf.load_model(compile_model = True)\n","### Come back later to check on this ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TuJn1U4oKM1z"},"source":["### Current alternative ### \n","# Use the same model defined above and load the weights independently\n","# Need to run the cell with the definition of the model class above once\n","model_class = BayesianCNN # CHANGE HERE\n","\n","kl_divergence_fn = (\n","    lambda q, p, _: (tfp.distributions.kl_divergence(q, p)) /\n","    tf.cast(clf.datahandler.X_train.shape[0], dtype=tf.float32)\n","    )\n","\n","clf.model = model_class(clf.datahandler.input_shape,\n","                        clf.datahandler.num_classes,\n","                        kl_divergence_fn)\n","\n","# LOAD WEIGHTS\n","weights_file = os.path.join(clf.logging.model_dir,\n","                            \"weights.h5\")\n","clf.model.load_weights(weights_file)\n","\n","# Compile and summarize the model\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanAbsoluteError, MeanSquaredError\n","from tensorflow.keras.losses import CategoricalCrossentropy\n","\n","learning_rate = 1e-04\n","optimizer = Adam(learning_rate = learning_rate) \n","\n","if clf.task == \"regression\":\n","    mae = MeanAbsoluteError()\n","    mse = MeanSquaredError()\n","    clf.model.compile(loss=mse,\n","                      optimizer=optimizer,\n","                      metrics=[\"mse\"])\n","    # =============================================================================\n","    # mse = MeanSquaredError()\n","    # clf.model.compile(loss = mse, optimizer = optimizer)\n","    # =============================================================================\n","    \n","elif clf.task == \"classification\":\n","    categorical_crossentropy = CategoricalCrossentropy()\n","    clf.model.compile(loss=categorical_crossentropy,\n","                      optimizer=optimizer,\n","                      metrics=[\"accuracy\",\n","                               \"categorical_crossentropy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"51UVlZILWxFW"},"source":["### Summarize the model"]},{"cell_type":"code","metadata":{"id":"KN24emZEWxFW"},"source":["# Plot summary and save model plot.\n","clf.summary()\n","clf.save_and_print_model_image()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFt_ILZ9EnDn"},"source":["### Show current weight distributions"]},{"cell_type":"code","metadata":{"id":"N-OgsDz2EnDn"},"source":["clf.plot_weight_distribution(kind=\"prior\", to_file=True)\n","clf.plot_weight_distribution(kind=\"posterior\", to_file=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hr2aMkdaqg2K"},"source":["### Show predictions with current model"]},{"cell_type":"code","metadata":{"id":"_4re_glXEnDo"},"source":["pred_train_intermediate, pred_test_intermediate = clf.predict()\n","\n","print('Train:')\n","for i in range(5):\n","    print('real: ' + str(np.round(y_train[i],3)),\n","          'pred: ' + str(pred_train_intermediate[i]))\n","print('Test:')\n","for i in range(5):\n","    print('real: ' + str(np.round(y_test[i],3)),\n","          'pred: ' + str(pred_test_intermediate[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z1pcEkr7EnDo"},"source":["no_of_predictions = 100\n","\n","prob_pred_test_intermediate = clf.predict_probabilistic(\n","    dataset=\"test\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_test_intermediate[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_test[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TcYkliU1WxFa"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"XVQWFw1yWxFa"},"source":["epochs = 1000\n","\n","new_learning_rate = 1e-03\n","validation_freq = 100\n","\n","hist = clf.train(checkpoint=True,\n","                 early_stopping=False,\n","                 tb_log=True, \n","                 csv_log=True,\n","                 hyperparam_log=True,\n","                 epochs=epochs, \n","                 batch_size=clf.logging.hyperparams['batch_size'],\n","                 validation_freq=validation_freq,\n","                 verbose=2,)\n","                 #new_learning_rate=new_learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VlkNFbYNWxFe"},"source":["### Plot loss"]},{"cell_type":"code","metadata":{"id":"9txM4QFyWxFe"},"source":["graph = clfutils.TrainingGraphs(clf.logging.history, clf.logging.fig_dir)\n","graph.plot_loss(to_file = True)\n","if clf.task == \"classification\":\n","    graph.plot_accuracy(to_file = False)\n","graph.plot_mse(to_file = True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OY7AxjWsWxFg"},"source":["### Evaluate on test data"]},{"cell_type":"code","metadata":{"id":"6deyORg8stjo","scrolled":true},"source":["if clf.task == 'classification':\n","    score = clf.evaluate()\n","    test_loss, test_accuracy = score[0], score[1]\n","    print('Test loss: ' + str(np.round(test_loss, decimals=8)))\n","    print('Test accuracy: ' + str(np.round(test_accuracy, decimals=3)))\n","elif clf.task == 'regression':\n","    test_loss = clf.evaluate()\n","    print('Test loss: ' + str(np.round(test_loss, decimals=8)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sAlwSDZNstjo"},"source":["###  Predict on train and test data"]},{"cell_type":"code","metadata":{"id":"DvBDN5ScFBND"},"source":["pred_train, pred_test = clf.predict()\n","if clf.task == 'classification':\n","    pred_train_classes, pred_test_classes = clf.predict_classes()\n","\n","print('Train:')\n","for i in range(5):\n","    print('real: ' + str(np.round(y_train[i],3)),\n","          'pred: ' + str(pred_train[i]))\n","print('Test:')\n","for i in range(5):\n","    print('real: ' + str(np.round(y_test[i],3)),\n","          'pred: ' + str(pred_test[i]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UOsQWaBeFBNE"},"source":["start = 0\n","stop = 5\n","no_of_predictions = 100\n","\n","prob_pred_test = clf.predict_probabilistic(\n","    dataset=\"test\",\n","    no_of_predictions=no_of_predictions\n",")\n","\n","for i, pred in enumerate(prob_pred_test[:10]):\n","   print(f\"Ground truth: {np.round(clf.datahandler.y_test[i],3)},\",\n","         f\"Mean prediction: {np.mean(pred, axis = 0)} +/- {np.std(pred, axis = 0)}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_oeZgBAkFBNE"},"source":["### Show some predictions"]},{"cell_type":"markdown","metadata":{"id":"bWWf9ORFFBNE"},"source":["#### 10 random training samples"]},{"cell_type":"code","metadata":{"id":"LCJXx9AvFBNE"},"source":["clf.plot_random(no_of_spectra=10, dataset='train', with_prediction=True)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SmI9ySIqFBNE"},"source":["#### 10 random test samples"]},{"cell_type":"code","metadata":{"id":"YIlKNEcrFBNF"},"source":["clf.plot_random(no_of_spectra=10, dataset='test', with_prediction=True)    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vN1IvEy0FBNF"},"source":["### Show wrong/worst predictions"]},{"cell_type":"code","metadata":{"cellView":"both","id":"pVwS0jiPFBNF"},"source":["if clf.task == 'classification':\n","    clf.show_wrong_classification()\n","elif clf.task == 'regression':\n","    clf.show_worst_predictions(no_of_spectra=20)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CCWyA2hSFBNF"},"source":["### Show posterior weight distribution after training update"]},{"cell_type":"code","metadata":{"id":"oQ0j20ezFBNF"},"source":["clf.plot_weight_distribution(kind=\"posterior\", to_file=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JGZhHmwVFBNF"},"source":["### Show distribution of probabilistic predictions"]},{"cell_type":"code","metadata":{"id":"xY7JJAQOFBNF"},"source":["clf.plot_prob_predictions(dataset=\"test\",\n","                          no_of_spectra=20,\n","                          to_file=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PL3Jn60BWxFq"},"source":["### Save model and data"]},{"cell_type":"code","metadata":{"id":"5Lt9sk16WxFr"},"source":["#clf.save_model()\n","clf.pickle_results()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nK9TOFCdWxFt"},"source":["### Generate report"]},{"cell_type":"code","metadata":{"id":"WKra_-rCWxFt"},"source":["dir_name = clf.time + '_' + clf.exp_name\n","rep = clfutils.Report(dir_name)  \n","rep.write()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZuYfPhUIOTO"},"source":["## Prepare website upload"]},{"cell_type":"code","metadata":{"id":"83dFzkLu3tkL"},"source":["from xpsdeeplearning.network.prepare_upload import Uploader\n","\n","dataset_path = clf.logging.hyperparams[\"input_filepath\"].rsplit(\".\",1)[0] + \"_metadata.json\"\n","uploader = Uploader(clf.logging.root_dir, dataset_path)\n","uploader.prepare_upload_params()\n","uploader.save_upload_params()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pk-dt26OLwHE"},"source":["## Save output of notebook"]},{"cell_type":"code","metadata":{"id":"mkSdyElDLwHF"},"source":["from IPython.display import Javascript, display\n","from nbconvert import HTMLExporter\n","\n","def save_notebook():\n","    display(Javascript(\"IPython.notebook.save_notebook()\"),\n","            include=['application/javascript'])\n","\n","def output_HTML(read_file, output_file):\n","    import codecs\n","    import nbformat\n","    exporter = HTMLExporter()\n","    # read_file is '.ipynb', output_file is '.html'\n","    output_notebook = nbformat.read(read_file, as_version=4)\n","    output, resources = exporter.from_notebook_node(output_notebook)\n","    codecs.open(output_file, 'w', encoding='utf-8').write(output)\n","\n","import time\n","import os\n","\n","time.sleep(20)\n","save_notebook()\n","print('Notebook saved!')\n","time.sleep(30)\n","current_file = '/content/drive/My Drive/deepxps/xpsdeeplearning/train_prob.ipynb'\n","output_file = os.path.join(clf.logging.log_dir,'train_prob_out.html')\n","output_HTML(current_file, output_file)\n","print('HTML file saved!')"],"execution_count":null,"outputs":[]}]}