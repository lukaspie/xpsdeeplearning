{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"train_multiple.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vt5hanGL4WpD"},"source":["# Train a deep CNN on XPS data on Google Colab"]},{"cell_type":"markdown","metadata":{"id":"uTjEnuR-LwEo"},"source":["In this notebook, we will train a deep convolutional network on iron XPS spectra made up of linear combinations of single iron reference spectra."]},{"cell_type":"markdown","metadata":{"id":"u6EVGrGFLwEr"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"dU2aEkqdLwE_"},"source":["### Mount google drive, change working directory"]},{"cell_type":"code","metadata":{"id":"5iL11_yXLwFB"},"source":["# Mount drive\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","\n","# Change working path\n","os.chdir('/content/drive/My Drive/deepxps')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rC3yXYXaLwEt"},"source":["### Install packages and import modules"]},{"cell_type":"code","metadata":{"id":"s4MxYB_b33V9"},"source":["%%capture\n","# Install packages\n","!pip install python-docx\n","!pip install tensorflow==2.3.0 as tf\n","\n","# Import standard modules and magic commands\n","import datetime\n","import numpy as np\n","import pytz\n","import importlib\n","\n","# Magic commands\n","%matplotlib inline\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\"\n","\n","# Disable tf warnings\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","\n","import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ad6lrbtPUwVk"},"source":["### Check TensorFlow version"]},{"cell_type":"code","metadata":{"id":"1wemeM47_Dsy"},"source":["tf.__version__"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rMuIKSLjUeBO"},"source":["### Check TPU connection"]},{"cell_type":"code","metadata":{"id":"2wcWtXa1UcSP"},"source":["from tensorflow.python.profiler import profiler_client\r\n","\r\n","tpu_profile_service_address = os.environ['COLAB_TPU_ADDR'].replace('8470', '8466')\r\n","print(profiler_client.monitor(tpu_profile_service_address, 100, 2))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R1BPxWcLLwFp"},"source":["## Initial training"]},{"cell_type":"markdown","metadata":{"id":"-qQx0QfuLwFQ"},"source":["### Load custom modules"]},{"cell_type":"code","metadata":{"id":"SfLJzbL04VZ8"},"source":["try:\n","    importlib.reload(classifier)\n","    importlib.reload(clfutils)\n","    print('Modules were reloaded.')\n","except:\n","    import xpsdeeplearning.network.classifier as classifier\n","    import xpsdeeplearning.network.utils as clfutils\n","    print('Modules were loaded.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j7M70DZczykg"},"source":["### Set up the parameters & folder structure\n","\n"]},{"cell_type":"code","metadata":{"id":"GXy2WdXYXcXc"},"source":["np.random.seed(502)\n","time = datetime.datetime.now().astimezone(pytz.timezone('Europe/Berlin')).strftime(\"%Y%m%d_%Hh%Mm\")\n","data_name = 'Fe_4_classes_linear_comb_new_noise_small_resnet'\n","label_values = ['Fe metal', 'FeO', 'Fe3O4', 'Fe2O3']\n","#label_values = ['Pd metal', 'PdO']\n","\n","clf = classifier.ClassifierMultiple(time = time,\n","                                    data_name = data_name,\n","                                    labels = label_values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q0wkRVOuLwFy"},"source":["### Load and inspect the data"]},{"cell_type":"code","metadata":{"id":"KZIiyOyKtxmf"},"source":["input_filepath = r'/content/drive/My Drive/deepxps/datasets/20201612_iron_variable_linear_combination_gas_phase_combined_data.h5'\n","train_test_split = 0.2\n","train_val_split = 0.2\n","no_of_examples = 100000\n","\n","X_train, X_val, X_test, y_train, y_val, y_test,\\\n","    aug_values_train, aug_values_val, aug_values_test =\\\n","        clf.load_data_preprocess(input_filepath = input_filepath,\n","                                 no_of_examples = no_of_examples,\n","                                 train_test_split = train_test_split,\n","                                 train_val_split = train_val_split)\n","               \n","# Check how the examples are distributed across the classes.\n","class_distribution = clf.check_class_distribution()\n","clf.plot_class_distribution()\n","clf.plot_random(no_of_spectra = 10, dataset = 'train')  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TKXmi1LMLwF6"},"source":["### Design the model"]},{"cell_type":"code","metadata":{"id":"pyxJ9_qh5awz"},"source":["try:\n","    importlib.reload(models)\n","    print('Models module was reloaded.')\n","except:\n","    import xpsdeeplearning.network.models as models\n","    print('Models module was loaded.')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zlav04xq9Xr6"},"source":["import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras.initializers import glorot_uniform\n","from tensorflow.python.keras import backend as K\n","\n","class CustomCNNMultiple(models.EmptyModel):\n","    \"\"\"\n","    A CNN with three convolutional layers of different kernel size at \n","    the beginning. Works well for learning across scales.\n","    \n","    This is to be used for regression on all labels. -> sigmoid \n","    activation in the last layer.\n","    \"\"\"\n","    def __init__(self, inputshape, num_classes):      \n","        self.input_1 = layers.Input(shape = inputshape)\n","        \n","        self.conv_1_short = layers.Conv1D(filters=12,\n","                                          kernel_size=5,\n","                                          strides=1,\n","                                          padding='same',\n","                                          activation='relu',\n","                                          name='conv_1_short')(self.input_1)\n","        self.conv_1_medium = layers.Conv1D(filters=12,\n","                                           kernel_size=10,\n","                                           strides=1,\n","                                           padding='same',\n","                                           activation='relu',\n","                                           name='conv_1_medium')(self.input_1)\n","        self.conv_1_long = layers.Conv1D(filters=12,\n","                                         kernel_size=10,\n","                                         strides=1,\n","                                         padding='same',\n","                                         activation='relu',\n","                                         name='conv_1_long')(self.input_1)\n","        \n","        sublayers = [self.conv_1_short, self.conv_1_medium, self.conv_1_long]\n","        merged_sublayers = layers.concatenate(sublayers)\n","\n","        self.conv_2 = layers.Conv1D(filters=10,\n","                                    kernel_size=5,\n","                                    strides=1,\n","                                    padding='same',\n","                                    activation='relu',\n","                                    name='conv_2')(merged_sublayers)\n","        self.conv_3 = layers.Conv1D(filters=10,\n","                                    kernel_size=5,\n","                                    strides=1,\n","                                    padding='same',\n","                                    activation='relu',\n","                                    name=\"conv_3\")(self.conv_2)\n","        self.average_pool_1 = layers.AveragePooling1D(\n","            name='average_pool_1')(self.conv_3)\n","        \n","        self.flatten_1 = layers.Flatten(name='flatten1')(self.average_pool_1)\n","        self.drop_1 = layers.Dropout(rate=0.2,\n","                                     name='drop_1')(self.flatten_1)\n","        self.dense_1 = layers.Dense(units=4000,\n","                                    activation='relu',\n","                                    name='dense_1')(self.drop_1)\n","        self.dense_2 = layers.Dense(units=num_classes,\n","                                    activation='sigmoid',\n","                                    name='dense_2')(self.dense_1)\n","        \n","        self.output_norm = layers.Lambda(\n","            lambda x: x/tf.reshape(K.sum(x, axis=-1),(-1,1)),\n","            name = 'output_normalization')(self.dense_2)\n","\n","        no_of_inputs = len(sublayers)\n","\n","        super(CustomCNNMultiple, self).__init__(inputs=self.input_1,\n","                                                outputs=self.output_norm,\n","                                                inputshape=inputshape,\n","                                                num_classes=num_classes,\n","                                                no_of_inputs=no_of_inputs,\n","                                                name='Custom_CNN_multiple') "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9_iGDpjOWhK"},"source":["# =============================================================================\n","# from tensorflow.keras import layers\n","# from tensorflow.keras.initializers import glorot_uniform\n","# from tensorflow.python.keras import backend as K\n","# \n","# \n","# class ResNet1D(models.EmptyModel):\n","#     \"\"\"\n","#     Instantiates the ResNet50 architecture in 1D similar to the original \n","#     ResNet paper. Using the functional API in Keras.\n","#     \n","#     Implementation of the popular ResNet50 the following architecture:\n","#     CONV1D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 ->\n","#     CONVBLOCK -> IDBLOCK*3 -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> \n","#     IDBLOCK*2 -> AVGPOOL (optional) -> OUTPUTLAYER\n","#     \"\"\"\n","#     def __init__(self,\n","#                  inputshape,\n","#                  num_classes,\n","#                  ap=False,\n","#                  no_of_inputs=1):\n","#         \"\"\"\n","#         Parameters\n","#         ----------\n","#         num_classes : int\n","#             Number of output classes.\n","#         ap : bool, optional\n","#             If ap, then an AveragePooling1D layer is added after the\n","#             residual blocks. The default is False.\n","#         no_of_inputs : int, optional\n","#             Number of input layers. The default is 1.\n","#             (not working here)\n","# \n","#         Returns\n","#         -------\n","#         None.\n","# \n","#         \"\"\"\n","#         self.ap = ap\n","#         \n","#         self.input_1 = layers.Input(shape=inputshape,\n","#                                     name='input_1')\n","#     \n","#         # Zero-Padding\n","#         self.zero_pad_1 = layers.ZeroPadding1D(padding=3)(self.input_1)\n","#         \n","#         # Stage 1\n","#         self.conv_1 = layers.Conv1D(filters=64,\n","#                                     kernel_size=2,\n","#                                     padding='valid',\n","#                                     kernel_initializer=glorot_uniform(seed=0),\n","#                                     name='stage1_conv')(self.zero_pad_1)\n","#         self.batch_1 = layers.BatchNormalization(\n","#             axis=1,\n","#             name='stage1_bn')(self.conv_1)\n","#         self.act_1 = layers.Activation(\n","#             activation='relu',\n","#             name='stage1_act')(self.batch_1)\n","#         self.max_pool_1 = layers.MaxPooling1D(\n","#             pool_size=1,\n","#             strides=1,\n","#             name='stage1_max_pool')(self.act_1)\n","#         \n","#         # Stage 2\n","#         self.conv_block_2a = models.ConvBlock(filters=[32,32,128],\n","#                                        kernel_size_2=4,\n","#                                        stage=2,\n","#                                        block='a')(self.max_pool_1)\n","#         self.id_block_2b = models.IdentityBlock(filters=[32,32,128],\n","#                                          kernel_size_2=1,\n","#                                          stage=2,\n","#                                          block='b')(self.conv_block_2a)\n","#         self.id_block_2c = models.IdentityBlock(filters=[32,32,128],\n","#                                          kernel_size_2=1,\n","#                                          stage=2,\n","#                                          block='c')(self.id_block_2b)\n","# \n","#         # Stage 3\n","#         self.conv_block_3a = models.ConvBlock(filters=[64,64,256],\n","#                                        kernel_size_2=3,\n","#                                        stage=3,\n","#                                        block='a')(self.id_block_2c)\n","#         self.id_block_3b = models.IdentityBlock(filters=[64,64,256],\n","#                                          kernel_size_2=3,\n","#                                          stage=3,\n","#                                          block='b')(self.conv_block_3a)\n","#         self.id_block_3c = models.IdentityBlock(filters=[64,64,256],\n","#                                          kernel_size_2=3,\n","#                                          stage=3,\n","#                                          block='c')(self.id_block_3b)\n","#         self.id_block_3d = models.IdentityBlock(filters=[64,64,256],\n","#                                          kernel_size_2=3,\n","#                                          stage=3,\n","#                                          block='d')(self.id_block_3c)\n","#         \n","# # =============================================================================\n","# #         # Stage 4\n","# #         self.conv_block_4a = models.ConvBlock(filters=[128,128,1024],\n","# #                                        kernel_size_2=3,\n","# #                                        stage=4,\n","# #                                        block='a')(self.id_block_3d)\n","# #         self.id_block_4b = models.IdentityBlock(filters=[128,128,1024],\n","# #                                          kernel_size_2=3,\n","# #                                          stage=4,\n","# #                                          block='b')(self.conv_block_4a)\n","# #         self.id_block_4c = models.IdentityBlock(filters=[128,128,1024],\n","# #                                          kernel_size_2=3,\n","# #                                          stage=4,\n","# #                                           block='c')(self.id_block_4b)\n","# #         self.id_block_4d = models.IdentityBlock(filters=[128,128,1024],\n","# #                                          kernel_size_2=3,\n","# #                                          stage=4,\n","# #                                          block='d')(self.id_block_4c)\n","# #         self.id_block_4e = models.IdentityBlock(filters=[128,128,1024],\n","# #                                          kernel_size_2=3,\n","# #                                          stage=4,\n","# #                                          block='e')(self.id_block_4d)\n","# #         self.id_block_4f = models.IdentityBlock(filters=[128,128,1024],\n","# #                                          kernel_size_2=3,\n","# #                                          stage=4,\n","# #                                          block='f')(self.id_block_4e)\n","# # \n","# #         # Stage 5\n","# #         self.conv_block_5a = models.ConvBlock(filters=[256,56,1024],\n","# #                                        kernel_size_2=1,\n","# #                                        stage=5,\n","# #                                        block='a')(self.id_block_4f)\n","# #         self.id_block_5b = models.IdentityBlock(filters=[256,56,1024],\n","# #                                          kernel_size_2=1,\n","# #                                          stage=5,\n","# #                                          block='b')(self.conv_block_5a)\n","# #         self.id_block_5c = models.IdentityBlock(filters=[256,56,1024],\n","# #                                          kernel_size_2=1,\n","# #                                          stage=5,\n","# #                                          block='c')(self.id_block_5b)\n","# # =============================================================================\n","# \n","#         # Average pooling\n","#         if self.ap:\n","#             self.avg_pool = layers.AveragePooling1D(\n","#                 pool_size=3,\n","#                 name='avg_pool')(self.id_block_3d)\n","#             self.flatten = layers.Flatten(name='flatten')(self.avg_pool)\n","#         \n","#         else:\n","#             self.flatten = layers.Flatten(name='flatten')(self.id_block_3d)\n","# \n","#         # output layer\n","#         self.dense = layers.Dense(units=num_classes,\n","#                                   activation='sigmoid',\n","#                                   kernel_initializer = glorot_uniform(seed=0),\n","#                                   name='dense')(self.flatten)\n","#         \n","#         # output norm\n","#         self.output_norm = layers.Lambda(\n","#             lambda x: x/tf.reshape(K.sum(x, axis=-1),(-1,1)),\n","#             name = 'output_norm')(self.dense)\n","#         \n","#         super(ResNet1D, self).__init__(inputs=self.input_1,\n","#                                         outputs=self.output_norm,\n","#                                         inputshape=inputshape,\n","#                                         num_classes=num_classes,\n","#                                         no_of_inputs=no_of_inputs, \n","#                                         name ='ResNet1D')\n","# =============================================================================\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"76OKygMa9Xr9"},"source":["### Build the model"]},{"cell_type":"code","metadata":{"id":"Af3wbjEW9Xr-"},"source":["clf.model = CustomCNNMultiple(clf.input_shape, clf.num_classes)\n","# =============================================================================\n","# clf.model = ResNet1D(clf.input_shape,\n","#                      clf.num_classes,\n","#                      ap=True)\n","# =============================================================================\n","# Alternative: Build model from available models\n","#clf.model = models.CustomCNNMultiple(clf.input_shape, clf.num_classes)\n","# =============================================================================\n","# clf.model = models.ResNet1D(clf.input_shape,\n","#                             clf.num_classes,\n","#                             ap=True)\n","# ============================================================================="],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I2SpBC-R9XsA"},"source":["### Compile and summarize the model"]},{"cell_type":"code","metadata":{"id":"vKQZDayn9XsB"},"source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n","\n","learning_rate = 1e-05\n","optimizer = Adam(learning_rate = learning_rate) \n","mse = MeanSquaredError()\n","mae = MeanAbsoluteError()\n","\n","# =============================================================================\n","# def custom_loss(y_true, y_pred):\n","#     \"\"\"\n","#     Custom loss for linear combination of XPS spectra.\n","#     \"\"\"\n","#     squared_difference = tf.square(tf.subtract(y_true,y_pred))\n","#     squared_output = tf.square(y_pred)\n","#     \n","#     return tf.reduce_sum(tf.multiply(squared_output,squared_difference))\n","# =============================================================================\n","\n","# Compile model with build-in loss function\n","#clf.model.compile(loss = mse, optimizer = optimizer)\n","clf.model.compile(loss = mae, optimizer = optimizer)\n","\n","# Plot summary and save model plot.\n","clf.summary()\n","clf.save_and_print_model_image()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oeboNYkNsf7Z"},"source":["### Show initial predictions"]},{"cell_type":"code","metadata":{"id":"Tpj40ttsCpEj"},"source":["pred_train_initial, pred_test_initial = clf.predict()\n","\n","print(y_train[:5])\n","print(pred_train_initial[:5])\n","\n","print(y_test[:5])\n","print(pred_test_initial[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GDOgVS_bLwGC"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"abV_fFCb5fiZ"},"source":["epochs = 1000\n","batch_size = 32\n","\n","hist = clf.train(checkpoint = True,\n","                 early_stopping = False,\n","                 tb_log = True, \n","                 csv_log = True,\n","                 epochs = epochs, \n","                 batch_size = batch_size,\n","                 verbose = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J9uYqsMlLwGJ"},"source":["### Plot loss"]},{"cell_type":"code","metadata":{"id":"Ffr2nfZy5nYi"},"source":["dir_name = clf.time + '_' + clf.data_name\n","graph = clfutils.TrainingGraphs(clf.history, dir_name)\n","graph.plot_loss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jnAnvSXLLwGQ"},"source":["### Evaluate on test data"]},{"cell_type":"code","metadata":{"id":"4g_QmUYG5fuT"},"source":["test_loss = clf.evaluate()\n","print('Test loss: ' + str(test_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2uCoVCI-LwGY"},"source":["###  Predict on train and test data"]},{"cell_type":"code","metadata":{"id":"ezklX5YY5fyr"},"source":["pred_train, pred_test = clf.predict()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nWbogghiLwGl"},"source":["### Show some predictions"]},{"cell_type":"markdown","metadata":{"id":"T5DGB_OGLwGm"},"source":["#### 10 random training samples"]},{"cell_type":"code","metadata":{"id":"keDKIJriLwGn"},"source":["clf.plot_random(no_of_spectra = 10, dataset = 'train', with_prediction = True)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-PHAS9XILwGr"},"source":["#### 10 random test samples"]},{"cell_type":"code","metadata":{"id":"HM3ZZf-qLwGs"},"source":["clf.plot_random(no_of_spectra = 10, dataset = 'test', with_prediction = True)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ROK1zo8xzBZF"},"source":["### Show worst predictions"]},{"cell_type":"code","metadata":{"cellView":"both","id":"uHFC5PWQzE19"},"source":["clf.show_worst_predictions(no_of_spectra = 20)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KwrgvaLpLwG3"},"source":["### Save model and data"]},{"cell_type":"code","metadata":{"id":"2HBdnZSq5f2D"},"source":["#clf.save_model()\n","clf.save_hyperparams()\n","clf.shelve_results(full = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c6B2wITlLwG_"},"source":["### Generate report"]},{"cell_type":"code","metadata":{"id":"-rPXD0ki5pOp"},"source":["dir_name = clf.time + '_' + clf.data_name\n","rep = clfutils.Report(dir_name)  \n","rep.write()   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K4mJKWJxWllD"},"source":["## Continue training"]},{"cell_type":"markdown","metadata":{"id":"VH95P7yXcCTq"},"source":["### Load custom modules"]},{"cell_type":"code","metadata":{"id":"ILECvnh6cCTr"},"source":["try:\n","    import importlib\n","    importlib.reload(classifier)\n","    importlib.reload(clfutils)\n","    print('\\n Modules were reloaded.')\n","except:\n","    import xpsdeeplearning.network.classifier as classifier\n","    import xpsdeeplearning.network.utils as clfutils\n","    print('\\n Modules were loaded.')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qu-oYRDYv93B"},"source":["### Set up the parameters & folder structure"]},{"cell_type":"code","metadata":{"id":"LvGQCtcLXF71"},"source":["np.random.seed(502)\n","time = '20210218_09h21m'\n","data_name = 'Fe_4_classes_linear_comb_new_noise_small_resnet'\n","\n","label_values = ['Fe metal', 'FeO', 'Fe3O4', 'Fe2O3']\n","#label_values = ['Pd metal', 'PdO']\n","clf = classifier.ClassifierMultiple(time = time,\n","                                    data_name = data_name,\n","                                    labels = label_values)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hO_5SSXX0O7I"},"source":["### Load and inspect the data"]},{"cell_type":"code","metadata":{"id":"bXXFcFUCWxFI"},"source":["input_filepath = r'/content/drive/My Drive/deepxps/datasets/20201612_iron_variable_linear_combination_gas_phase_combined_data.h5'\n","train_test_split = 0.2\n","train_val_split = 0.2\n","no_of_examples = 100000\n","\n","X_train, X_val, X_test, y_train, y_val, y_test,\\\n","    aug_values_train, aug_values_val, aug_values_test =\\\n","        clf.load_data_preprocess(input_filepath = input_filepath,\n","                                 no_of_examples = no_of_examples,\n","                                 train_test_split = train_test_split,\n","                                 train_val_split = train_val_split)\n","                \n","# Check how the examples are distributed across the classes.\n","class_distribution = clf.check_class_distribution()\n","clf.plot_class_distribution()\n","clf.plot_random(no_of_spectra = 10, dataset = 'train')  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BqJ1CeG1WxFM"},"source":["### Load the model"]},{"cell_type":"code","metadata":{"id":"JvzitnVNWxFR"},"source":["clf.load_model()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"51UVlZILWxFW"},"source":["### Compile and summarize the model"]},{"cell_type":"code","metadata":{"id":"KN24emZEWxFW"},"source":["from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError\n","\n","learning_rate = 1e-05\n","optimizer = Adam(learning_rate = learning_rate) \n","mse = MeanSquaredError()\n","mae = MeanAbsoluteError()\n","# =============================================================================\n","# def custom_loss(y_true, y_pred):\n","#     \"\"\"\n","#     Custom loss for linear combination of XPS spectra.\n","#     \"\"\"\n","#     squared_difference = tf.square(tf.subtract(y_true,y_pred))\n","#     squared_output = tf.square(y_pred)\n","#     \n","#     return tf.reduce_sum(tf.multiply(squared_output,squared_difference))\n","# =============================================================================\n","\n","# Compile model with build-in loss function\n","#clf.model.compile(loss = mse, optimizer = optimizer)\n","clf.model.compile(loss = mae, optimizer = optimizer)\n","\n","# Plot summary and save model plot.\n","clf.summary()\n","clf.save_and_print_model_image()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hr2aMkdaqg2K"},"source":["### Show predictions with current model"]},{"cell_type":"code","metadata":{"id":"YXWyQ_cHqIJ2"},"source":["pred_train_intermediate, pred_test_intermediate = clf.predict()\r\n","\r\n","print(y_train[:5])\r\n","print(pred_train_intermediate[:5])\r\n","\r\n","print(y_test[:5])\r\n","print(pred_test_intermediate[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TcYkliU1WxFa"},"source":["### Train"]},{"cell_type":"code","metadata":{"id":"XVQWFw1yWxFa"},"source":["epochs = 200\n","batch_size = 32\n","\n","# =============================================================================\n","# hist = clf.train(checkpoint = True,\n","#                  early_stopping = False,\n","#                  tb_log = True, \n","#                  csv_log = True,\n","#                  epochs = epochs, \n","#                  batch_size = batch_size,\n","#                  verbose = 1)\n","# =============================================================================\n","\n","new_learning_rate = 5e-06\n","\n","hist = clf.train(checkpoint = True,\n","                 early_stopping = False,\n","                 tb_log = True, \n","                 csv_log = True,\n","                 epochs = epochs, \n","                 batch_size = batch_size,\n","                 verbose = 1)#, \n","#                 new_learning_rate = new_learning_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-RrWBcNuP67"},"source":["from tensorflow.keras import backend as K\n","print('New learning rate: ' +\\\n","      str(K.eval(clf.model.optimizer.lr)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VlkNFbYNWxFe"},"source":["### Plot loss"]},{"cell_type":"code","metadata":{"id":"9txM4QFyWxFe"},"source":["dir_name = clf.time + '_' + clf.data_name\n","graph = clfutils.TrainingGraphs(clf.history, dir_name)\n","graph.plot_loss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OY7AxjWsWxFg"},"source":["### Evaluate on test data"]},{"cell_type":"code","metadata":{"id":"ww82F0GhWxFg"},"source":["test_loss = clf.evaluate()\n","print('Test loss: ' + str(test_loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yp09B1quWxFi"},"source":["###  Predict on train & test data"]},{"cell_type":"code","metadata":{"id":"TigHLl7OWxFj"},"source":["pred_train, pred_test = clf.predict()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HyWjEwJoWxFk"},"source":["### Show some predictions"]},{"cell_type":"markdown","metadata":{"id":"pV7aAEMQWxFl"},"source":["#### 10 random training samples"]},{"cell_type":"code","metadata":{"id":"T6BgD25qWxFl"},"source":["clf.plot_random(no_of_spectra = 10, dataset = 'train', with_prediction = True)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18CPy-pqWxFo"},"source":["#### 10 random test samples"]},{"cell_type":"code","metadata":{"id":"Mlad8aaEWxFo"},"source":["clf.plot_random(no_of_spectra = 10, dataset = 'test', with_prediction = True)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1u-ocIdb1_1Z"},"source":["### Show worst predictions"]},{"cell_type":"code","metadata":{"id":"zxM3ctcf1_1b"},"source":["clf.show_worst_predictions(no_of_spectra = 20)  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PL3Jn60BWxFq"},"source":["### Save model and data"]},{"cell_type":"code","metadata":{"id":"5Lt9sk16WxFr"},"source":["#clf.save_model()\n","clf.save_hyperparams()\n","clf.shelve_results(full = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nK9TOFCdWxFt"},"source":["### Generate report"]},{"cell_type":"code","metadata":{"id":"WKra_-rCWxFt"},"source":["dir_name = clf.time + '_' + clf.data_name\n","rep = clfutils.Report(dir_name)  \n","rep.write()   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pk-dt26OLwHE"},"source":["## Save output of notebook"]},{"cell_type":"code","metadata":{"id":"mkSdyElDLwHF"},"source":["from IPython.display import Javascript, display\n","from nbconvert import HTMLExporter\n","\n","def save_notebook():\n","    display(Javascript(\"IPython.notebook.save_notebook()\"),\n","            include=['application/javascript'])\n","\n","def output_HTML(read_file, output_file):\n","    import codecs\n","    import nbformat\n","    exporter = HTMLExporter()\n","    # read_file is '.ipynb', output_file is '.html'\n","    output_notebook = nbformat.read(read_file, as_version=4)\n","    output, resources = exporter.from_notebook_node(output_notebook)\n","    codecs.open(output_file, 'w', encoding='utf-8').write(output)\n","\n","import time\n","import os\n","\n","time.sleep(20)\n","save_notebook()\n","print('Notebook saved!')\n","time.sleep(30)\n","current_file = '/content/drive/My Drive/app/xpsdeeplearning/train_multiple.ipynb'\n","output_file = os.path.join(clf.log_dir,'train_multiple_out.html')\n","output_HTML(current_file, output_file)\n","print('HTML file saved!')"],"execution_count":null,"outputs":[]}]}